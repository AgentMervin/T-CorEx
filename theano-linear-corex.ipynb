{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install CUDA and cudamat (for python) to enable GPU speedups.\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from scipy.stats import norm, rankdata\n",
    "\n",
    "import lasagne\n",
    "\n",
    "import gc\n",
    "try:\n",
    "    import cudamat as cm\n",
    "    GPU_SUPPORT = True\n",
    "except:\n",
    "    print(\"Install CUDA and cudamat (for python) to enable GPU speedups.\")\n",
    "    GPU_SUPPORT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g(x, t=4):\n",
    "    \"\"\"A transformation that suppresses outliers for a standard normal.\"\"\"\n",
    "    xp = np.clip(x, -t, t)\n",
    "    diff = np.tanh(x - xp)\n",
    "    return xp + diff\n",
    "\n",
    "\n",
    "def g_inv(x, t=4):\n",
    "    \"\"\"Inverse of g transform.\"\"\"\n",
    "    xp = np.clip(x, -t, t)\n",
    "    diff = np.arctanh(np.clip(x - xp, -1 + 1e-10, 1 - 1e-10))\n",
    "    return xp + diff\n",
    "\n",
    "def mean_impute(x, v):\n",
    "    \"\"\"Missing values in the data, x, are indicated by v. Wherever this value appears in x, it is replaced by the\n",
    "    mean value taken from the marginal distribution of that column.\"\"\"\n",
    "    if not np.isnan(v):\n",
    "        x = np.where(x == v, np.nan, x)\n",
    "    x_new = []\n",
    "    n_obs = []\n",
    "    for i, xi in enumerate(x.T):\n",
    "        missing_locs = np.where(np.isnan(xi))[0]\n",
    "        xi_nm = xi[np.isfinite(xi)]\n",
    "        xi[missing_locs] = np.mean(xi_nm)\n",
    "        x_new.append(xi)\n",
    "        n_obs.append(len(xi_nm))\n",
    "    return np.array(x_new).T, np.array(n_obs)\n",
    "\n",
    "\n",
    "def random_impute(x, v):\n",
    "    \"\"\"Missing values in the data, x, are indicated by v. Wherever this value appears in x, it is replaced by a\n",
    "    random value taken from the marginal distribution of that column.\"\"\"\n",
    "    if not np.isnan(v):\n",
    "        x = np.where(x == v, np.nan, x)\n",
    "    x_new = []\n",
    "    for i, xi in enumerate(x.T):\n",
    "        missing_locs = np.where(np.isnan(xi))[0]\n",
    "        xi_nm = xi[np.isfinite(xi)]\n",
    "        xi[missing_locs] = np.random.choice(xi_nm, size=len(missing_locs))\n",
    "        x_new.append(xi)\n",
    "    return np.array(x_new).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-4-02a9b19c5c4a>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-02a9b19c5c4a>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    def _define_model(self, anneal_eps):\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Corex:\n",
    "\n",
    "     def __init__(self, ns, nv, n_hidden=10, max_iter=10000, tol=1e-5, anneal=True, missing_values=None,\n",
    "                 discourage_overlap=True, gaussianize='standard', gpu=False, yscale=1.0,\n",
    "                 verbose=False, seed=None):\n",
    "        self.ns = ns  # Number of samplesse\n",
    "        self.n_sample = ns  # Some functions use this name\n",
    "        self.nv = nv  # Number of variables\n",
    "        self.m = n_hidden  # Number of latent factors to learn\n",
    "        self.max_iter = max_iter  # Number of iterations to try\n",
    "        self.tol = tol  # Threshold for convergence\n",
    "        self.anneal = anneal\n",
    "        self.eps = 0  # If anneal is True, it's adjusted during optimization to avoid local minima\n",
    "        self.missing_values = missing_values\n",
    "\n",
    "        self.discourage_overlap = discourage_overlap  # Whether or not to discourage overlapping latent factors\n",
    "        self.gaussianize = gaussianize  # Preprocess data: 'standard' scales to zero mean and unit variance\n",
    "        self.gpu = gpu  # Enable GPU support for some large matrix multiplications.\n",
    "        if self.gpu:\n",
    "            cm.cublas_init()\n",
    "\n",
    "        self.yscale = yscale  # Can be arbitrary, but sets the scale of Y\n",
    "        np.random.seed(seed)  # Set seed for deterministic results\n",
    "        self.verbose = verbose\n",
    "        if verbose:\n",
    "            np.set_printoptions(precision=3, suppress=True, linewidth=160)\n",
    "            print('Linear CorEx with {:d} latent factors'.format(n_hidden))\n",
    "\n",
    "        self.moments = {}  # Dictionary of moments\n",
    "        self.theta = None  # Parameters for preprocessing each variable\n",
    "        self.history = {}  # Keep track of values for each iteration\n",
    "\n",
    "    def _define_model(self, anneal_eps):\n",
    "        self.x_wno = T.matrix('X')\n",
    "        self.anneal_noise = RandomStreams.normal(size=(self.ns, self.nv))\n",
    "        self.x = np.sqrt(1 - anneal_eps ** 2) * self.x_wno + anneal_eps * self.anneal_noise\n",
    "\n",
    "        self.z_noise = RandomStreams.normal(avg=0.0, std=yscale, size=(self.ns, self.m))\n",
    "        self.ws = theano.shared(np.random.randn(self.m, self.nv).astype(np.float32), name='W')\n",
    "        self.z_mean = T.dot(x, self.ws.T)\n",
    "        self.z = self.z_mean + self.z_noise\n",
    "\n",
    "        z2 = (z ** 2).mean(axis=0)  # j\n",
    "        R = T.dot(z.T, x) / ns  # j, i\n",
    "        R = R / T.sqrt(z2)  # as <x^2_i> == 1 we don't divide by it\n",
    "\n",
    "        ri = ((R ** 2) / (1 - R ** 2)).sum(axis=0)\n",
    "\n",
    "        # vxiz\n",
    "        outer_term = (1 / (1 + ri)).reshape((1, self.nv))\n",
    "        inner_term_1 = ((R ** 2) / (1 - R ** 2)).reshape((1, self.m, self.nv))\n",
    "        inner_term_2 = self.z.reshape((self.ns, self.m, 1))\n",
    "        inner_term = (inner_term_1 * inner_term_2).sum(axis=1)  # (ns, nv)\n",
    "        vxiz = outer_term * inner_term  # (ns, nv)\n",
    "\n",
    "        # objective\n",
    "        obj_part_1 = 0.5 * T.log(((self.x - vxiz) ** 2).mean(axis=0)).sum(axis=0)\n",
    "        obj_part_2 = 0.5 * T.log(z2).mean(axis=0).sum(axis=0)\n",
    "        self.obj = obj_part_1 + obj_part_2\n",
    "\n",
    "        # optimizer\n",
    "        updates = lasagne.updates.adam(self.obj, self.ws)\n",
    "\n",
    "        # functions\n",
    "        self.train_step = theano.function(inputs=[self.x_wno],\n",
    "                                          outputs=[self.obj, self.z_mean],\n",
    "                                          updates=updates)\n",
    "        self.predict = theano.function(inputs=[self.x_wno],\n",
    "                                       outputs=[self.z_mean])\n",
    "\n",
    "    def fit(self, x):\n",
    "        x = np.asarray(x, dtype=np.float32)\n",
    "        x = self.preprocess(x, fit=True)  # Fit a transform for each marginal\n",
    "        assert x.shape[0] == self.ns\n",
    "        assert x.shape[1] == self.nv\n",
    "\n",
    "        anneal_schedule = [0.]\n",
    "        if self.anneal:\n",
    "            anneal_schedule = [0.6**k for k in range(1, 7)] + [0]\n",
    "        self.moments = self._calculate_moments(x, self.ws, quick=True)\n",
    "\n",
    "        for i_eps, eps in enumerate(anneal_schedule):\n",
    "            self.eps = eps\n",
    "            if i_eps > 0:\n",
    "                old_ws = self.ws.get_value()\n",
    "                self._define_model(eps)\n",
    "                self.ws.set_value(old_ws)\n",
    "            else:\n",
    "                self._define_model(eps)\n",
    "            \n",
    "            self._update_U(x)  \n",
    "            self.moments = self._calculate_moments(x, self.ws, quick=True)\n",
    "\n",
    "            for i_loop in range(self.max_iter):\n",
    "                last_tc = self.tc  # Save this TC to compare to possible updates\n",
    "                (obj, _) = self.train_step(x)\n",
    "                self._update_U(x)\n",
    "                \n",
    "                if not self.moments or not np.isfinite(self.tc):\n",
    "                    try:\n",
    "                        print((\"Error: TC is no longer finite: {}\".format(self.tc)))\n",
    "                    except:\n",
    "                        print(\"Error... updates giving invalid solutions?\")\n",
    "                        return self\n",
    "                delta = np.abs(self.tc - last_tc)\n",
    "                self.update_records(self.moments, delta)  # Book-keeping\n",
    "                if delta < self.tol:  # Check for convergence\n",
    "                    if self.verbose:\n",
    "                        print(('{:d} iterations to tol: {:f}'.format(i_loop, self.tol)))\n",
    "                    break\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print((\"Warning: Convergence not achieved in {:d} iterations. \"\n",
    "                          \"Final delta: {:f}\".format(self.max_iter, delta.sum())))\n",
    "        self.moments = self._calculate_moments(x, self.ws, quick=False)  # Update moments with details\n",
    "        order = np.argsort(-self.moments[\"TCs\"])  # Largest TC components first.\n",
    "        \n",
    "        ws = self.ws.get_value()\n",
    "        ws = ws[order]\n",
    "        self.ws.set_value(ws)\n",
    "        self._udapte_U(x)\n",
    "        \n",
    "        self.moments = self._calculate_moments(x, self.ws, quick=False)  # Update moments based on sorted weights.\n",
    "        return self\n",
    "    \n",
    "    def _update_U(self, x):\n",
    "        W = self.ws.get_value()\n",
    "        self.U = self.getU(W, x)\n",
    "\n",
    "    def update_records(self, moments, delta):\n",
    "        \"\"\"Print and store some statistics about each iteration.\"\"\"\n",
    "        gc.disable()  # There's a bug that slows when appending, fixed by temporarily disabling garbage collection\n",
    "        self.history[\"TC\"] = self.history.get(\"TC\", []) + [moments[\"TC\"]]\n",
    "        if self.verbose > 1:\n",
    "            print((\"TC={:.3f}\\tadd={:.3f}\\tdelta={:.6f}\".format(moments[\"TC\"], moments.get(\"additivity\", 0), delta)))\n",
    "        if self.verbose:\n",
    "            self.history[\"additivity\"] = self.history.get(\"additivity\", []) + [moments.get(\"additivity\", 0)]\n",
    "            self.history[\"TCs\"] = self.history.get(\"TCs\", []) + [moments.get(\"TCs\", np.zeros(self.m))]\n",
    "        gc.enable()\n",
    "\n",
    "    @property\n",
    "    def tc(self):\n",
    "        \"\"\"This actually returns the lower bound on TC that is optimized. The lower bound assumes a constraint that\n",
    "         would be satisfied by a non-overlapping model.\n",
    "         Check \"moments\" for two other estimates of TC that may be useful.\"\"\"\n",
    "        return self.moments[\"TC\"]\n",
    "\n",
    "    @property\n",
    "    def tcs(self):\n",
    "        \"\"\"TCs for each individual latent factor. They DO NOT sum to TC in this case, because of overlaps.\"\"\"\n",
    "        return self.moments[\"TCs\"]\n",
    "\n",
    "    @property\n",
    "    def mis(self):\n",
    "        return - 0.5 * np.log1p(-self.moments[\"rho\"]**2)\n",
    "\n",
    "    def clusters(self):\n",
    "        return np.argmax(np.abs(self.U), axis=0)  # TODO: understand this\n",
    "\n",
    "    def _sig(self, x, u):\n",
    "        \"\"\"Multiple the matrix u by the covariance matrix of x. We are interested in situations where\n",
    "        n_variables >> n_samples, so we do this without explicitly constructing the covariance matrix.\"\"\"\n",
    "        if self.gpu:\n",
    "            y = cm.empty((self.n_samples, self.m))\n",
    "            uc = cm.CUDAMatrix(u)\n",
    "            cm.dot(x, uc.T, target=y)\n",
    "            del uc\n",
    "            tmp = cm.empty((self.nv, self.m))\n",
    "            cm.dot(x.T, y, target=tmp)\n",
    "            tmp_dot = tmp.asarray()\n",
    "            del y\n",
    "            del tmp\n",
    "        else:\n",
    "            y = x.dot(u.T)\n",
    "            tmp_dot = x.T.dot(y)\n",
    "        prod = (1 - self.eps**2) * tmp_dot.T / self.n_samples + self.eps**2 * u  # nv by m,  <X_i Y_j> / std Y_j\n",
    "        return prod\n",
    "    \n",
    "    def getW(self, U, x):\n",
    "        # W_{ji} = U_{ji} * \\sqrt{E[Z_j^2]}\n",
    "        \"\"\"\n",
    "        W = np.zeros(U.shape)\n",
    "        for j in range(U.shape[0]):\n",
    "            W[j, :] = U[j, :] * np.sqrt(self.z2_fromU(j, U, x))\n",
    "        return W\n",
    "        \"\"\"\n",
    "        tmp_dot = np.dot(self._sig(x, U), U.T)\n",
    "        z2 = (self.yscale**2) / (1 - np.einsum(\"ii->i\", tmp_dot))\n",
    "        return U * np.sqrt(z2).reshape((-1, 1))\n",
    "\n",
    "    def getU(self, W, x):\n",
    "        # U_{ji} = \\frac{W_{ji}}{\\sqrt{E[Z_j^2]}}\n",
    "        \"\"\"\n",
    "        U = np.zeros(W.shape)\n",
    "        for j in range(W.shape[0]):\n",
    "            U[j, :] = W[j, :] / np.sqrt(self.z2_fromW(j, W, x))\n",
    "        return U\n",
    "        \"\"\"\n",
    "        tmp_dot = np.dot(self._sig(x, W), W.T)\n",
    "        z2 = self.yscale**2 + np.einsum(\"ii->i\", tmp_dot)\n",
    "        return W / np.sqrt(z2).reshape((-1, 1))\n",
    "\n",
    "    def _calculate_moments(self, x, ws, quick=False):\n",
    "        if self.discourage_overlap:\n",
    "            return self._calculate_moments_ns(x, self.U, quick=quick)\n",
    "        else:\n",
    "            return self._calculate_moments_syn(x, self.U, quick=quick)\n",
    "\n",
    "    def _calculate_moments_ns(self, x, ws, quick=False):\n",
    "        \"\"\"Calculate moments based on the weights and samples. We also calculate and save MI, TC, additivity, and\n",
    "        the value of the objective. Note it is assumed that <X_i^2> = 1! \"\"\"\n",
    "        m = {}  # Dictionary of moments\n",
    "        if self.gpu:\n",
    "            y = cm.empty((self.n_samples, self.m))\n",
    "            wc = cm.CUDAMatrix(ws)\n",
    "            cm.dot(x, wc.T, target=y)  # + noise, but it is included analytically\n",
    "            del wc\n",
    "            tmp_sum = np.einsum('lj,lj->j', y.asarray(), y.asarray())  # TODO: Should be able to do on gpu...\n",
    "        else:\n",
    "            y = x.dot(ws.T)\n",
    "            tmp_sum = np.einsum('lj,lj->j', y, y)\n",
    "        m[\"uj\"] = (1 - self.eps**2) * tmp_sum / self.n_samples + self.eps**2 * np.sum(ws**2, axis=1)\n",
    "        if quick and np.max(m[\"uj\"]) >= 1.:\n",
    "            return False\n",
    "        if self.gpu:\n",
    "            tmp = cm.empty((self.nv, self.m))\n",
    "            cm.dot(x.T, y, target=tmp)\n",
    "            tmp_dot = tmp.asarray()\n",
    "            del tmp\n",
    "            del y\n",
    "        else:\n",
    "            tmp_dot = x.T.dot(y)\n",
    "        m[\"rho\"] = (1 - self.eps**2) * tmp_dot.T / self.n_samples + self.eps**2 * ws  # m by nv\n",
    "        m[\"ry\"] = ws.dot(m[\"rho\"].T)  # normalized covariance of Y\n",
    "        m[\"Y_j^2\"] = self.yscale ** 2 / (1. - m[\"uj\"])\n",
    "        np.fill_diagonal(m[\"ry\"], 1)\n",
    "        m[\"invrho\"] = 1. / (1. - m[\"rho\"]**2)\n",
    "        m[\"rhoinvrho\"] = m[\"rho\"] * m[\"invrho\"]\n",
    "        m[\"Qij\"] = np.dot(m['ry'], m[\"rhoinvrho\"])\n",
    "        m[\"Qi\"] = np.einsum('ki,ki->i', m[\"rhoinvrho\"], m[\"Qij\"])\n",
    "        #m[\"Qi-Si^2\"] = np.einsum('ki,ki->i', m[\"rhoinvrho\"], m[\"Qij\"])\n",
    "        m[\"Si\"] = np.sum(m[\"rho\"] * m[\"rhoinvrho\"], axis=0)\n",
    "\n",
    "        # This is the objective, a lower bound for TC\n",
    "        m[\"TC\"] = np.sum(np.log(1 + m[\"Si\"])) \\\n",
    "                     - 0.5 * np.sum(np.log(1 - m[\"Si\"]**2 + m[\"Qi\"])) \\\n",
    "                     + 0.5 * np.sum(np.log(1 - m[\"uj\"]))\n",
    "\n",
    "        if not quick:\n",
    "            m[\"MI\"] = - 0.5 * np.log1p(-m[\"rho\"]**2)\n",
    "            m[\"X_i Y_j\"] = m[\"rho\"].T * np.sqrt(m[\"Y_j^2\"])\n",
    "            m[\"X_i Z_j\"] = np.linalg.solve(m[\"ry\"], m[\"rho\"]).T\n",
    "            m[\"X_i^2 | Y\"] = (1. - np.einsum('ij,ji->i', m[\"X_i Z_j\"], m[\"rho\"])).clip(1e-6)\n",
    "            m['I(Y_j ; X)'] = 0.5 * np.log(m[\"Y_j^2\"]) - 0.5 * np.log(self.yscale ** 2)\n",
    "            m['I(X_i ; Y)'] = - 0.5 * np.log(m[\"X_i^2 | Y\"])\n",
    "            m[\"TCs\"] = m[\"MI\"].sum(axis=1) - m['I(Y_j ; X)']\n",
    "            m[\"TC_no_overlap\"] = m[\"MI\"].max(axis=0).sum() - m['I(Y_j ; X)'].sum()  # A direct calculation of TC where each variable is in exactly one group.\n",
    "            m[\"TC_direct\"] = m['I(X_i ; Y)'].sum() - m['I(Y_j ; X)']  # A direct calculation of TC. Should be upper bound for \"TC\", \"TC_no_overlap\"\n",
    "            m[\"additivity\"] = (m[\"MI\"].sum(axis=0) - m['I(X_i ; Y)']).sum()\n",
    "        return m\n",
    "\n",
    "    def _calculate_moments_syn(self, x, ws, quick=False):\n",
    "        \"\"\"Calculate moments based on the weights and samples. We also calculate and save MI, TC, additivity, and\n",
    "        the value of the objective. Note it is assumed that <X_i^2> = 1! \"\"\"\n",
    "        m = {}  # Dictionary of moments\n",
    "        if self.gpu:\n",
    "            y = cm.empty((self.n_samples, self.m))\n",
    "            wc = cm.CUDAMatrix(ws)\n",
    "            cm.dot(x, wc.T, target=y)  # + noise, but it is included analytically\n",
    "            del wc\n",
    "        else:\n",
    "            y = x.dot(ws.T)  # + noise, but it is included analytically\n",
    "        if self.gpu:\n",
    "            tmp_dot = cm.empty((self.nv, self.m))\n",
    "            cm.dot(x.T, y, target=tmp_dot)\n",
    "            m[\"X_i Y_j\"] = tmp_dot.asarray() / self.n_samples  # nv by m,  <X_i Y_j>\n",
    "            del y\n",
    "            del tmp_dot\n",
    "        else:\n",
    "            m[\"X_i Y_j\"] = x.T.dot(y) / self.n_samples\n",
    "        m[\"cy\"] = ws.dot(m[\"X_i Y_j\"]) + self.yscale ** 2 * np.eye(self.m)  # cov(y.T), m by m\n",
    "        m[\"Y_j^2\"] = np.diag(m[\"cy\"]).copy()\n",
    "        m[\"ry\"] = m[\"cy\"] / (np.sqrt(m[\"Y_j^2\"]) * np.sqrt(m[\"Y_j^2\"][:, np.newaxis]))\n",
    "        m[\"rho\"] = (m[\"X_i Y_j\"] / np.sqrt(m[\"Y_j^2\"])).T\n",
    "        m[\"invrho\"] = 1. / (1. - m[\"rho\"]**2)\n",
    "        m[\"rhoinvrho\"] = m[\"rho\"] * m[\"invrho\"]\n",
    "        m[\"Qij\"] = np.dot(m['ry'], m[\"rhoinvrho\"])\n",
    "        m[\"Qi\"] = np.einsum('ki,ki->i', m[\"rhoinvrho\"], m[\"Qij\"])\n",
    "        m[\"Si\"] = np.sum(m[\"rho\"] * m[\"rhoinvrho\"], axis=0)\n",
    "\n",
    "        m[\"MI\"] = - 0.5 * np.log1p(-m[\"rho\"]**2)\n",
    "        m[\"X_i Z_j\"] = np.linalg.solve(m[\"cy\"], m[\"X_i Y_j\"].T).T\n",
    "        m[\"X_i^2 | Y\"] = (1. - np.einsum('ij,ij->i', m[\"X_i Z_j\"], m[\"X_i Y_j\"])).clip(1e-6)\n",
    "        mi_yj_x = 0.5 * np.log(m[\"Y_j^2\"]) - 0.5 * np.log(self.yscale ** 2)\n",
    "        mi_xi_y = - 0.5 * np.log(m[\"X_i^2 | Y\"])\n",
    "        m[\"TCs\"] = m[\"MI\"].sum(axis=1) - mi_yj_x\n",
    "        m[\"additivity\"] = (m[\"MI\"].sum(axis=0) - mi_xi_y).sum()\n",
    "        m[\"TC\"] = np.sum(mi_xi_y) - np.sum(mi_yj_x)\n",
    "        return m\n",
    "\n",
    "    def transform(self, x, details=False):\n",
    "        \"\"\"Transform an array of inputs, x, into an array of k latent factors, Y.\n",
    "            Optionally, you can get the remainder information and/or stop at a specified level.\"\"\"\n",
    "        x = self.preprocess(x)\n",
    "        ns, nv = x.shape\n",
    "        assert self.nv == nv, \"Incorrect number of variables in input, %d instead of %d\" % (nv, self.nv)\n",
    "        if details:\n",
    "            moments = self._calculate_moments(x, self.U)\n",
    "            return x.dot(self.U.T), moments\n",
    "        return x.dot(self.U.T)\n",
    "\n",
    "        \n",
    "    def preprocess(self, x, fit=False):\n",
    "        \"\"\"Transform each marginal to be as close to a standard Gaussian as possible.\n",
    "        'standard' (default) just subtracts the mean and scales by the std.\n",
    "        'empirical' does an empirical gaussianization (but this cannot be inverted).\n",
    "        'outliers' tries to squeeze in the outliers\n",
    "        Any other choice will skip the transformation.\"\"\"\n",
    "        if self.missing_values is not None:\n",
    "            x, self.n_obs = mean_impute(x, self.missing_values)  # Creates a copy\n",
    "        else:\n",
    "            self.n_obs = len(x)\n",
    "        if self.gaussianize == 'none':\n",
    "            pass\n",
    "        elif self.gaussianize == 'standard':\n",
    "            if fit:\n",
    "                mean = np.mean(x, axis=0)\n",
    "                # std = np.std(x, axis=0, ddof=0).clip(1e-10)\n",
    "                std = np.sqrt(np.sum((x - mean)**2, axis=0) / self.n_obs).clip(1e-10)\n",
    "                self.theta = (mean, std)\n",
    "            x = ((x - self.theta[0]) / self.theta[1])\n",
    "            if np.max(np.abs(x)) > 6 and self.verbose:\n",
    "                print(\"Warning: outliers more than 6 stds away from mean. Consider using gaussianize='outliers'\")\n",
    "        elif self.gaussianize == 'outliers':\n",
    "            if fit:\n",
    "                mean = np.mean(x, axis=0)\n",
    "                std = np.std(x, axis=0, ddof=0).clip(1e-10)\n",
    "                self.theta = (mean, std)\n",
    "            x = g((x - self.theta[0]) / self.theta[1])  # g truncates long tails\n",
    "        elif self.gaussianize == 'empirical':\n",
    "            print(\"Warning: correct inversion/transform of empirical gauss transform not implemented.\")\n",
    "            x = np.array([norm.ppf((rankdata(x_i) - 0.5) / len(x_i)) for x_i in x.T]).T\n",
    "        if self.gpu and fit:  # Don't return GPU matrices when only transforming\n",
    "            x = cm.CUDAMatrix(x)\n",
    "        return x\n",
    "    \n",
    "        def invert(self, x):\n",
    "        \"\"\"Invert the preprocessing step to get x's in the original space.\"\"\"\n",
    "        if self.gaussianize == 'standard':\n",
    "            return self.theta[1] * x + self.theta[0]\n",
    "        elif self.gaussianize == 'outliers':\n",
    "            return self.theta[1] * g_inv(x) + self.theta[0]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def predict(self, y):\n",
    "        return self.invert(np.dot(self.moments[\"X_i Z_j\"], y.T).T)\n",
    "\n",
    "    def get_covariance(self):\n",
    "        # This uses E(Xi|Y) formula for non-synergistic relationships\n",
    "        m = self.moments\n",
    "        if self.discourage_overlap:\n",
    "            z = m['rhoinvrho'] / (1 + m['Si'])\n",
    "            cov = np.dot(z.T, z)\n",
    "            cov /= (1. - self.eps**2)\n",
    "            np.fill_diagonal(cov, 1)\n",
    "            return self.theta[1][:, np.newaxis] * self.theta[1] * cov\n",
    "        else:\n",
    "            cov = np.einsum('ij,kj->ik', m[\"X_i Z_j\"], m[\"X_i Y_j\"])\n",
    "            np.fill_diagonal(cov, 1)\n",
    "            return self.theta[1][:, np.newaxis] * self.theta[1] * cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
