{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \n",
    "    \"\"\"Adam optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        \n",
    "        allowed_kwargs = {'clipnorm', 'clipvalue'}\n",
    "        for k in kwargs:\n",
    "            if k not in allowed_kwargs:\n",
    "                raise TypeError('Unexpected keyword argument '\n",
    "                                'passed to optimizer: ' + str(k))\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.iterations = 0\n",
    "        self.lr = lr\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    def get_update(self, params, grads):\n",
    "        \"\"\" params and grads are list of numpy arrays\n",
    "        \"\"\"\n",
    "        original_shapes = [x.shape for x in params]\n",
    "        params = [x.flatten() for x in params]\n",
    "        grads = [x.flatten() for x in grads]\n",
    "        \n",
    "        \"\"\" #TODO: implement clipping\n",
    "        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n",
    "            norm = np.sqrt(sum([np.sum(np.square(g)) for g in grads]))\n",
    "            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n",
    "        if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n",
    "            grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n",
    "        \"\"\"\n",
    "        \n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "        lr_t = lr * (np.sqrt(1. - np.power(self.beta_2, t)) /\n",
    "                     (1. - np.power(self.beta_1, t)))\n",
    "\n",
    "        if not hasattr(self, 'ms'):\n",
    "            self.ms = [np.zeros(p.shape) for p in params]\n",
    "            self.vs = [np.zeros(p.shape) for p in params]\n",
    "    \n",
    "        ret = [None] * len(params)\n",
    "        for i, p, g, m, v in zip(range(len(params)), params, grads, self.ms, self.vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * np.square(g)\n",
    "            p_t = p - lr_t * m_t / (np.sqrt(v_t) + self.epsilon)\n",
    "            self.ms[i] = m_t\n",
    "            self.vs[i] = v_t\n",
    "            ret[i] = p_t\n",
    "        \n",
    "        self.iterations += 1\n",
    "        \n",
    "        for i in range(len(ret)):\n",
    "            ret[i] = ret[i].reshape(original_shapes[i])\n",
    "        \n",
    "        return ret\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"SGD optimizer.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, **kwargs):\n",
    "        \n",
    "        allowed_kwargs = {'clipnorm', 'clipvalue'}\n",
    "        for k in kwargs:\n",
    "            if k not in allowed_kwargs:\n",
    "                raise TypeError('Unexpected keyword argument '\n",
    "                                'passed to optimizer: ' + str(k))\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.lr = lr\n",
    "\n",
    "    def get_update(self, params, grads):\n",
    "        \"\"\" params and grads are list of numpy arrays\n",
    "        \"\"\"\n",
    "        original_shapes = [x.shape for x in params]\n",
    "        params = [x.flatten() for x in params]\n",
    "        grads = [x.flatten() for x in grads]\n",
    "        \n",
    "        \"\"\" #TODO: implement clipping\n",
    "        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n",
    "            norm = np.sqrt(sum([np.sum(np.square(g)) for g in grads]))\n",
    "            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n",
    "        if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n",
    "            grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n",
    "        \"\"\"\n",
    "        \n",
    "        ret = [None] * len(params)\n",
    "        for i, p, g in zip(range(len(params)), params, grads):\n",
    "            ret[i] = p - self.lr * g\n",
    "        \n",
    "        for i in range(len(ret)):\n",
    "            ret[i] = ret[i].reshape(original_shapes[i])\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install CUDA and cudamat (for python) to enable GPU speedups.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Linear Total Correlation Explanation\n",
    "Recovers linear latent factors from data, like PCA/ICA/FA, etc. except that\n",
    "these factors are maximally informative about relationships in the data.\n",
    "We also constrain our solutions to be \"non-synergistic\" for better interpretability.\n",
    "(That is the TC(Y|Xi)=0 constraint in the \"blessing of dimensionality\" paper.)\n",
    "Code below written by:\n",
    "Greg Ver Steeg (gregv@isi.edu), 2017.\n",
    "\"\"\"\n",
    "\n",
    "from scipy.stats import norm, rankdata\n",
    "import gc\n",
    "try:\n",
    "    import cudamat as cm\n",
    "    GPU_SUPPORT = True\n",
    "except:\n",
    "    print(\"Install CUDA and cudamat (for python) to enable GPU speedups.\")\n",
    "    GPU_SUPPORT = False\n",
    "\n",
    "\n",
    "class Corex(object):\n",
    "    \"\"\"\n",
    "    Linear Total Correlation Explanation\n",
    "    Conventions\n",
    "    ----------\n",
    "    Code follows sklearn naming/style (e.g. fit(X) to train, transform() to apply model to test data).\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_hidden : int, default = 2\n",
    "        The number of latent factors to use.\n",
    "    max_iter : int, default = 10000\n",
    "        The max. number of iterations to reach convergence.\n",
    "    tol : float, default = 0.0001\n",
    "        Used to test for convergence.\n",
    "    eliminate_synergy : bool, default = True\n",
    "        Use a constraint that the information latent factors have about data is not synergistic.\n",
    "    gaussianize : str, default = 'standard'\n",
    "        Preprocess data so each marginal is near a standard normal. See gaussianize method for more details.\n",
    "    yscale : float default = 1\n",
    "        We imagine some small fundamental measurement noise on Y. The value is arbitrary, but it sets\n",
    "        the scale of the results, Y.\n",
    "    verbose : int, optional\n",
    "        Print verbose outputs.\n",
    "    seed : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    References\n",
    "    ----------\n",
    "    [1] Greg Ver Steeg and Aram Galstyan. \"Maximally Informative Hierarchical...\", AISTATS 2015.\n",
    "    [2] Greg Ver Steeg, Shuyang Gao, Kyle Reing, and Aram Galstyan. \"Sifting Common Information from Many Variables\",\n",
    "                                                                    IJCAI 2017.\n",
    "    [3] Greg Ver Steeg and Aram Galstyan. \"Low Complexity Gaussian Latent Factor Models and\n",
    "                                           a Blessing of Dimensionality\", 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_hidden=10, max_iter=10000, tol=1e-5, anneal=True, missing_values=None,\n",
    "                 discourage_overlap=True, gaussianize='standard', gpu=False,\n",
    "                 verbose=False, seed=None, optimizer=SGD(lr=1e-2)):\n",
    "    \n",
    "        self.m = n_hidden  # Number of latent factors to learn\n",
    "        self.max_iter = max_iter  # Number of iterations to try\n",
    "        self.tol = tol  # Threshold for convergence\n",
    "        self.anneal = anneal\n",
    "        self.eps = 0  # If anneal is True, it's adjusted during optimization to avoid local minima\n",
    "        self.missing_values = missing_values\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.discourage_overlap = discourage_overlap  # Whether or not to discourage overlapping latent factors\n",
    "        self.gaussianize = gaussianize  # Preprocess data: 'standard' scales to zero mean and unit variance\n",
    "        self.gpu = gpu  # Enable GPU support for some large matrix multiplications.\n",
    "        if self.gpu:\n",
    "            cm.cublas_init()\n",
    "\n",
    "        self.yscale = 1.  # Can be arbitrary, but sets the scale of Y\n",
    "        np.random.seed(seed)  # Set seed for deterministic results\n",
    "        self.verbose = verbose\n",
    "        if verbose:\n",
    "            np.set_printoptions(precision=3, suppress=True, linewidth=160)\n",
    "            print('Linear CorEx with {:d} latent factors'.format(n_hidden))\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.fit(x)\n",
    "        return self.transform(x)\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.nt = len(x)  #$ number of timesteps\n",
    "        self.n_samples = [a.shape[0] for a in x]  #$ number of samples of each timestep\n",
    "        self.nv = x[0].shape[1]  #$ number of input variables\n",
    "\n",
    "        self.moments = [{} for t in range(self.nt)]  # Dictionary of moments\n",
    "        self.history = [{} for t in range(self.nt)]  # Keep track of values for each iteration\n",
    "\n",
    "        x = [np.array(a, dtype=np.float32) for a in x]\n",
    "        x = self.preprocess(x, fit=True)  # Fit a transform for each marginal\n",
    "\n",
    "        if self.m is None:\n",
    "            self.m = pick_n_hidden(x)\n",
    "        anneal_schedule = [0.]\n",
    "        if not hasattr(self, 'ws'):  # Randomly initialize weights if not already set\n",
    "            if self.discourage_overlap:\n",
    "                self.ws = np.random.randn(self.nt, self.m, self.nv).astype(np.float32)\n",
    "                for t in range(self.nt):\n",
    "                    self.ws[t] /= (10. * self._norm(x[t], self.ws[t]))[:, np.newaxis]  # TODO: test good IC\n",
    "                if self.anneal:\n",
    "                    anneal_schedule = [0.6**k for k in range(1, 7)] + [0]\n",
    "            else:\n",
    "                self.ws = np.random.randn(self.nt, self.m, self.nv) * self.yscale ** 2 / np.sqrt(self.nv)\n",
    "        self.moments = self._calculate_moments(x, self.ws, quick=True)\n",
    "\n",
    "        for i_eps, eps in enumerate(anneal_schedule):\n",
    "            start_time = time.time()\n",
    "            self.eps = eps\n",
    "            if i_eps > 0:\n",
    "                eps0 = anneal_schedule[i_eps - 1]\n",
    "                for t in range(self.nt):\n",
    "                    mag = (1 - self.yscale**2 / self.moments[t]['Y_j^2']).clip(1e-5)  # May be better to re-initialize un-used latent factors (i.e. yj^2=self.yscale**2)?\n",
    "                    wmag = np.sum(self.ws[t]**2, axis=1)\n",
    "                    self.ws[t] *= np.sqrt((1 - eps0**2) / (1 - eps**2 - (eps0**2 - eps**2) * wmag / mag))[:, np.newaxis]\n",
    "            self.moments = self._calculate_moments(x, self.ws, quick=True)\n",
    "\n",
    "            for i_loop in range(self.max_iter):\n",
    "                last_tc = np.sum(self.tc)  # Save this TC to compare to possible updates\n",
    "                if self.discourage_overlap:\n",
    "                    self.ws, self.moments = self._update_ns(x)\n",
    "                else:\n",
    "                    self.ws, self.moments = self._update_syn(x, eta=0.1)  # Older method that allows synergies\n",
    "\n",
    "                # assert np.isfinite(self.tc), \"Error: TC is no longer finite: {}\".format(self.tc)\n",
    "                if not self.moments or not np.isfinite(np.sum(self.tc)):\n",
    "                    try:\n",
    "                        print(\"Error: TC is no longer finite: {}\".format(np.sum(self.tc)))\n",
    "                    except:\n",
    "                        print(\"Error... updates giving invalid solutions?\")\n",
    "                        return self\n",
    "                delta = np.abs(np.sum(self.tc) - last_tc)\n",
    "                self.update_records(self.moments, delta)  # Book-keeping\n",
    "                if delta < self.tol:  # Check for convergence\n",
    "                    if self.verbose:\n",
    "                        print('{:d} iterations to tol: {:f}, Time: {}'.format(i_loop, self.tol,\n",
    "                                                                              time.time() - start_time))\n",
    "                    break\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(\"Warning: Convergence not achieved in {:d} iterations. \"\n",
    "                          \"Final delta: {:f}, Time: {}\".format(self.max_iter, delta.sum(),\n",
    "                                                               time.time() - start_time))\n",
    "        self.moments = self._calculate_moments(x, self.ws, quick=False)  # Update moments with details\n",
    "        for t in range(self.nt):\n",
    "            order = np.argsort(-self.moments[t][\"TCs\"])  # Largest TC components first.\n",
    "            self.ws[t] = self.ws[t][order]\n",
    "        self.moments = self._calculate_moments(x, self.ws, quick=False)  # Update moments based on sorted weights.\n",
    "        return self\n",
    "\n",
    "    def update_records(self, moments, delta):\n",
    "        \"\"\"Print and store some statistics about each iteration.\"\"\"\n",
    "        gc.disable()  # There's a bug that slows when appending, fixed by temporarily disabling garbage collection\n",
    "        for t in range(self.nt):\n",
    "            self.history[t][\"TC\"] = self.history[t].get(\"TC\", []) + [moments[t][\"TC\"]]\n",
    "        if self.verbose > 1:\n",
    "            tc_sum = sum([m[\"TC\"] for m in moments])\n",
    "            add_sum = sum([m.get(\"additivity\", 0) for m in moments])\n",
    "            print(\"TC={:.3f}\\tadd={:.3f}\\tdelta={:.6f}\".format(tc_sum, add_sum, delta))\n",
    "        if self.verbose:\n",
    "            for t in range(self.nt):\n",
    "                self.history[t][\"additivity\"] = self.history[t].get(\"additivity\", []) + [moments[t].get(\"additivity\", 0)]\n",
    "                self.history[t][\"TCs\"] = self.history[t].get(\"TCs\", []) + [moments[t].get(\"TCs\", np.zeros(self.m))]\n",
    "        gc.enable()\n",
    "\n",
    "    @property\n",
    "    def tc(self):\n",
    "        \"\"\"This actually returns the lower bound on TC that is optimized. The lower bound assumes a constraint that\n",
    "         would be satisfied by a non-overlapping model.\n",
    "         Check \"moments\" for two other estimates of TC that may be useful.\"\"\"\n",
    "        return [m[\"TC\"] for m in self.moments]\n",
    "\n",
    "    @property\n",
    "    def tcs(self):\n",
    "        \"\"\"TCs for each individual latent factor. They DO NOT sum to TC in this case, because of overlaps.\"\"\"\n",
    "        return [m[\"TCs\"] for m in self.moments]\n",
    "\n",
    "    @property\n",
    "    def mis(self):\n",
    "        return [-0.5 * np.log1p(-m[\"rho\"]**2) for m in self.moments]\n",
    "\n",
    "    def clusters(self):\n",
    "        return np.argmax(np.abs(self.ws), axis=1)\n",
    "\n",
    "    def _sig(self, x, u):\n",
    "        \"\"\"Multiple the matrix u by the covariance matrix of x. We are interested in situations where\n",
    "        n_variables >> n_samples, so we do this without explicitly constructing the covariance matrix.\"\"\"\n",
    "        n_samples = len(x)\n",
    "        if self.gpu:\n",
    "            y = cm.empty((n_samples, self.m))\n",
    "            uc = cm.CUDAMatrix(u)\n",
    "            cm.dot(x, uc.T, target=y)\n",
    "            del uc\n",
    "            tmp = cm.empty((self.nv, self.m))\n",
    "            cm.dot(x.T, y, target=tmp)\n",
    "            tmp_dot = tmp.asarray()\n",
    "            del y\n",
    "            del tmp\n",
    "        else:\n",
    "            y = x.dot(u.T)\n",
    "            tmp_dot = x.T.dot(y)\n",
    "        prod = (1 - self.eps**2) * tmp_dot.T / n_samples + self.eps**2 * u  # nv by m,  <X_i Y_j> / std Y_j\n",
    "        return prod\n",
    "\n",
    "    def _norm(self, x, ws):\n",
    "        \"\"\"Calculate uj so that we can normalize it.\"\"\"\n",
    "        n_samples = len(x)\n",
    "        if self.gpu:\n",
    "            y = cm.empty((self.n_samples, self.m))\n",
    "            wc = cm.CUDAMatrix(ws)\n",
    "            cm.dot(x, wc.T, target=y)  # + noise, but it is included analytically\n",
    "            y_local = y.asarray()\n",
    "            del y\n",
    "            del wc\n",
    "            tmp_sum = np.einsum('lj,lj->j', y_local, y_local)  # TODO: Should be able to do on gpu...\n",
    "        else:\n",
    "            y = x.dot(ws.T)  # + noise / std Y_j^2, but it is included analytically\n",
    "            tmp_sum = np.einsum('lj,lj->j', y, y)\n",
    "        return np.sqrt((1 - self.eps**2) * tmp_sum / n_samples + self.eps**2 * np.sum(ws**2, axis=1))\n",
    "\n",
    "    def _calculate_moments(self, x, ws, quick=False):\n",
    "        ret = [None] * self.nt\n",
    "        for t in range(self.nt):\n",
    "            if self.discourage_overlap:\n",
    "                ret[t] = self._calculate_moments_ns(x[t], ws[t], quick=quick)\n",
    "            else:\n",
    "                ret[t] = self._calculate_moments_syn(x[t], ws[t], quick=quick)\n",
    "        return ret\n",
    "\n",
    "    def _calculate_moments_ns(self, x, ws, quick=False):\n",
    "        \"\"\"Calculate moments based on the weights and samples. We also calculate and save MI, TC, additivity, and\n",
    "        the value of the objective. Note it is assumed that <X_i^2> = 1! \"\"\"\n",
    "        m = {}  # Dictionary of moments\n",
    "        n_samples = len(x)\n",
    "        if self.gpu:\n",
    "            y = cm.empty((self.n_samples, self.m))\n",
    "            wc = cm.CUDAMatrix(ws)\n",
    "            cm.dot(x, wc.T, target=y)  # + noise, but it is included analytically\n",
    "            del wc\n",
    "            tmp_sum = np.einsum('lj,lj->j', y.asarray(), y.asarray())  # TODO: Should be able to do on gpu...\n",
    "        else:\n",
    "            y = x.dot(ws.T)\n",
    "            tmp_sum = np.einsum('lj,lj->j', y, y)\n",
    "        m[\"uj\"] = (1 - self.eps**2) * tmp_sum / n_samples + self.eps**2 * np.sum(ws**2, axis=1)\n",
    "#         if quick and np.max(m[\"uj\"]) >= 1.:\n",
    "#             return False\n",
    "        if self.gpu:\n",
    "            tmp = cm.empty((self.nv, self.m))\n",
    "            cm.dot(x.T, y, target=tmp)\n",
    "            tmp_dot = tmp.asarray()\n",
    "            del tmp\n",
    "            del y\n",
    "        else:\n",
    "            tmp_dot = x.T.dot(y)\n",
    "        m[\"rho\"] = (1 - self.eps**2) * tmp_dot.T / n_samples + self.eps**2 * ws  # m by nv\n",
    "        m[\"ry\"] = ws.dot(m[\"rho\"].T)  # normalized covariance of Y\n",
    "        m[\"Y_j^2\"] = self.yscale ** 2 / (1. - m[\"uj\"])\n",
    "        np.fill_diagonal(m[\"ry\"], 1)\n",
    "        m[\"invrho\"] = 1. / (1. - m[\"rho\"]**2)\n",
    "        m[\"rhoinvrho\"] = m[\"rho\"] * m[\"invrho\"]\n",
    "        m[\"Qij\"] = np.dot(m['ry'], m[\"rhoinvrho\"])\n",
    "        m[\"Qi\"] = np.einsum('ki,ki->i', m[\"rhoinvrho\"], m[\"Qij\"])\n",
    "        #m[\"Qi-Si^2\"] = np.einsum('ki,ki->i', m[\"rhoinvrho\"], m[\"Qij\"])\n",
    "        m[\"Si\"] = np.sum(m[\"rho\"] * m[\"rhoinvrho\"], axis=0)\n",
    "\n",
    "        # This is the objective, a lower bound for TC\n",
    "        m[\"TC\"] = np.sum(np.log(1 + m[\"Si\"])) \\\n",
    "                     - 0.5 * np.sum(np.log(1 - m[\"Si\"]**2 + m[\"Qi\"])) \\\n",
    "                     + 0.5 * np.sum(np.log(1 - m[\"uj\"]))\n",
    "\n",
    "        if not quick:\n",
    "            m[\"MI\"] = - 0.5 * np.log1p(-m[\"rho\"]**2)\n",
    "            m[\"X_i Y_j\"] = m[\"rho\"].T * np.sqrt(m[\"Y_j^2\"])\n",
    "            m[\"X_i Z_j\"] = np.linalg.solve(m[\"ry\"], m[\"rho\"]).T\n",
    "            m[\"X_i^2 | Y\"] = (1. - np.einsum('ij,ji->i', m[\"X_i Z_j\"], m[\"rho\"])).clip(1e-6)\n",
    "            m['I(Y_j ; X)'] = 0.5 * np.log(m[\"Y_j^2\"]) - 0.5 * np.log(self.yscale ** 2)\n",
    "            m['I(X_i ; Y)'] = - 0.5 * np.log(m[\"X_i^2 | Y\"])\n",
    "            m[\"TCs\"] = m[\"MI\"].sum(axis=1) - m['I(Y_j ; X)']\n",
    "            m[\"TC_no_overlap\"] = m[\"MI\"].max(axis=0).sum() - m['I(Y_j ; X)'].sum()  # A direct calculation of TC where each variable is in exactly one group.\n",
    "            m[\"TC_direct\"] = m['I(X_i ; Y)'].sum() - m['I(Y_j ; X)']  # A direct calculation of TC. Should be upper bound for \"TC\", \"TC_no_overlap\"\n",
    "            m[\"additivity\"] = (m[\"MI\"].sum(axis=0) - m['I(X_i ; Y)']).sum()\n",
    "        return m\n",
    "\n",
    "    def z2_fromW(self, j, W, x):\n",
    "        # \\eta^2 + W_j \\Sigma W_j^T\n",
    "        return self.yscale**2 + np.dot(self._sig(x, W[j:j+1,:]), W[j:j+1, :].T)\n",
    "\n",
    "    def z2_fromU(self, j, U, x):\n",
    "        # \\frac{\\eta^2}{1 - U_j \\Sigma U_j^T}\n",
    "        return (self.yscale**2) / (1 - np.dot(self._sig(x, U[j:j+1,:]), U[j:j+1, :].T))\n",
    "\n",
    "    def getW(self, U, x):\n",
    "        # W_{ji} = U_{ji} * \\sqrt{E[Z_j^2]}\n",
    "        \"\"\"\n",
    "        W = np.zeros(U.shape)\n",
    "        for j in range(U.shape[0]):\n",
    "            W[j, :] = U[j, :] * np.sqrt(self.z2_fromU(j, U, x))\n",
    "        return W\n",
    "        \"\"\"\n",
    "        tmp_dot = np.dot(self._sig(x, U), U.T)\n",
    "        z2 = (self.yscale**2) / (1 - np.einsum(\"ii->i\", tmp_dot))\n",
    "        return U * np.sqrt(z2).reshape((-1, 1))\n",
    "\n",
    "    def getU(self, W, x):\n",
    "        # U_{ji} = \\frac{W_{ji}}{\\sqrt{E[Z_j^2]}}\n",
    "        \"\"\"\n",
    "        U = np.zeros(W.shape)\n",
    "        for j in range(W.shape[0]):\n",
    "            U[j, :] = W[j, :] / np.sqrt(self.z2_fromW(j, W, x))\n",
    "        return U\n",
    "        \"\"\"\n",
    "        tmp_dot = np.dot(self._sig(x, W), W.T)\n",
    "        z2 = self.yscale**2 + np.einsum(\"ii->i\", tmp_dot)\n",
    "        return W / np.sqrt(z2).reshape((-1, 1))\n",
    "    \n",
    "    def _update_ns(self, x):\n",
    "        #$ DONE\n",
    "        \"\"\"Perform one update of the weights and re-calculate moments in the NON-SYNERGISTIC case.\"\"\"\n",
    "        m = self.moments\n",
    "        w_updates = []\n",
    "        m_updates = []\n",
    "        \n",
    "        params = []\n",
    "        grads = []\n",
    "        \n",
    "        for t in range(self.nt):\n",
    "            rj = 1. - m[t][\"uj\"][:, np.newaxis]\n",
    "            H = np.dot(m[t][\"rhoinvrho\"] / (1 + m[t][\"Qi\"] - m[t][\"Si\"]**2), m[t][\"rhoinvrho\"].T)\n",
    "            np.fill_diagonal(H, 0)\n",
    "            grad = self.ws[t] / rj\n",
    "            grad -= 2 * m[t][\"invrho\"] * m[t][\"rhoinvrho\"] / (1 + m[t][\"Si\"])\n",
    "            grad += m[t][\"invrho\"]**2 * \\\n",
    "                   ((1 + m[t][\"rho\"]**2) * m[t][\"Qij\"] - 2 * m[t][\"rho\"] * m[t][\"Si\"]) / (1 - m[t][\"Si\"]**2 + m[t][\"Qi\"])\n",
    "            grad += np.dot(H, self.ws[t])\n",
    "            \n",
    "            # make a better gradient approximation by multipying each row by sqrt(E[zj^2])\n",
    "            \"\"\"\n",
    "            for j in range(self.ws[t].shape[0]):\n",
    "                z2 = np.float(self.z2_fromU(j, self.ws[t], x[t]))\n",
    "                grad[j, :] *= np.sqrt(z2)\n",
    "            \"\"\"\n",
    "            tmp_dot = np.dot(self._sig(x[t], self.ws[t]), self.ws[t].T)\n",
    "            z2 = (self.yscale**2) / (1 - np.einsum(\"ii->i\", tmp_dot))\n",
    "            grad *= np.sqrt(z2).reshape((-1, 1))\n",
    "            \n",
    "            current_real_W = self.getW(self.ws[t], x[t])\n",
    "            params.append(current_real_W)\n",
    "            grads.append(grad)\n",
    "        \n",
    "        new_real_Ws = self.optimizer.get_update(params, grads)\n",
    "        w_updates = [self.getU(new_real_Ws[t], x[t])\n",
    "                        for t in range(self.nt)]\n",
    "        m_updates = [self._calculate_moments_ns(x[t], w_updates[t], quick=True)\n",
    "                        for t in range(self.nt)]\n",
    "        \n",
    "        #print \"TC = {}, eps = {}\".format(self.tc, self.eps)\n",
    "        \n",
    "        return np.array(w_updates), m_updates\n",
    "\n",
    "    def _calculate_moments_syn(self, x, ws, quick=False):\n",
    "        #$ NO CHANGE\n",
    "        \"\"\"Calculate moments based on the weights and samples. We also calculate and save MI, TC, additivity, and\n",
    "        the value of the objective. Note it is assumed that <X_i^2> = 1! \"\"\"\n",
    "        m = {}  # Dictionary of moments\n",
    "        if self.gpu:\n",
    "            y = cm.empty((self.n_samples, self.m))\n",
    "            wc = cm.CUDAMatrix(ws)\n",
    "            cm.dot(x, wc.T, target=y)  # + noise, but it is included analytically\n",
    "            del wc\n",
    "        else:\n",
    "            y = x.dot(ws.T)  # + noise, but it is included analytically\n",
    "        if self.gpu:\n",
    "            tmp_dot = cm.empty((self.nv, self.m))\n",
    "            cm.dot(x.T, y, target=tmp_dot)\n",
    "            m[\"X_i Y_j\"] = tmp_dot.asarray() / self.n_samples  # nv by m,  <X_i Y_j>\n",
    "            del y\n",
    "            del tmp_dot\n",
    "        else:\n",
    "            m[\"X_i Y_j\"] = x.T.dot(y) / self.n_samples\n",
    "        m[\"cy\"] = ws.dot(m[\"X_i Y_j\"]) + self.yscale ** 2 * np.eye(self.m)  # cov(y.T), m by m\n",
    "        m[\"Y_j^2\"] = np.diag(m[\"cy\"]).copy()\n",
    "        m[\"ry\"] = m[\"cy\"] / (np.sqrt(m[\"Y_j^2\"]) * np.sqrt(m[\"Y_j^2\"][:, np.newaxis]))\n",
    "        m[\"rho\"] = (m[\"X_i Y_j\"] / np.sqrt(m[\"Y_j^2\"])).T\n",
    "        m[\"invrho\"] = 1. / (1. - m[\"rho\"]**2)\n",
    "        m[\"rhoinvrho\"] = m[\"rho\"] * m[\"invrho\"]\n",
    "        m[\"Qij\"] = np.dot(m['ry'], m[\"rhoinvrho\"])\n",
    "        m[\"Qi\"] = np.einsum('ki,ki->i', m[\"rhoinvrho\"], m[\"Qij\"])\n",
    "        m[\"Si\"] = np.sum(m[\"rho\"] * m[\"rhoinvrho\"], axis=0)\n",
    "\n",
    "        m[\"MI\"] = - 0.5 * np.log1p(-m[\"rho\"]**2)\n",
    "        m[\"X_i Z_j\"] = np.linalg.solve(m[\"cy\"], m[\"X_i Y_j\"].T).T\n",
    "        m[\"X_i^2 | Y\"] = (1. - np.einsum('ij,ij->i', m[\"X_i Z_j\"], m[\"X_i Y_j\"])).clip(1e-6)\n",
    "        mi_yj_x = 0.5 * np.log(m[\"Y_j^2\"]) - 0.5 * np.log(self.yscale ** 2)\n",
    "        mi_xi_y = - 0.5 * np.log(m[\"X_i^2 | Y\"])\n",
    "        m[\"TCs\"] = m[\"MI\"].sum(axis=1) - mi_yj_x\n",
    "        m[\"additivity\"] = (m[\"MI\"].sum(axis=0) - mi_xi_y).sum()\n",
    "        m[\"TC\"] = np.sum(mi_xi_y) - np.sum(mi_yj_x)\n",
    "        return m\n",
    "\n",
    "    def _update_syn(self, x, eta=0.5):\n",
    "        #$ DONE\n",
    "        \"\"\"Perform one update of the weights and re-calculate moments in the SYNERGISTIC case.\"\"\"\n",
    "        m = self.moments\n",
    "        ws = [None] * self.nt\n",
    "        for t in range(self.nt):\n",
    "            H = (1. / m[t][\"X_i^2 | Y\"] * m[t][\"X_i Z_j\"].T).dot(m[t][\"X_i Z_j\"])\n",
    "            np.fill_diagonal(H, 0)\n",
    "            R = m[t][\"X_i Z_j\"].T / m[t][\"X_i^2 | Y\"]\n",
    "            S = np.dot(H, self.ws[t])\n",
    "            ws[t] = (1. - eta) * self.ws[t] + eta * (R - S)\n",
    "            m[t] = self._calculate_moments_syn(x[t], ws[t])\n",
    "        return ws, m\n",
    "\n",
    "    def transform(self, x, details=False):\n",
    "        #$ DONE\n",
    "        \"\"\"Transform an array of inputs, x, into an array of k latent factors, Y.\n",
    "            Optionally, you can get the remainder information and/or stop at a specified level.\"\"\"\n",
    "        x = self.preprocess(x)\n",
    "        nt, ns, nv = len(x), x[0].shape\n",
    "        assert self.nv == nv, \"Incorrect number of variables in input, %d instead of %d\" % (nv, self.nv)\n",
    "        ret = [a.dot(w.T) for (a, w) in zip(x, self.ws)]\n",
    "        if details:\n",
    "            moments = self._calculate_moments(x, self.ws)\n",
    "            return ret, moments\n",
    "        return ret\n",
    "\n",
    "    def preprocess(self, X, fit=False):\n",
    "        #$ DONE\n",
    "        \"\"\"Transform each marginal to be as close to a standard Gaussian as possible.\n",
    "        'standard' (default) just subtracts the mean and scales by the std.\n",
    "        'empirical' does an empirical gaussianization (but this cannot be inverted).\n",
    "        'outliers' tries to squeeze in the outliers\n",
    "        Any other choice will skip the transformation.\"\"\"\n",
    "        if fit:\n",
    "            self.theta = []  #$ clear self.theta\n",
    "        for t in range(len(X)):\n",
    "            x = X[t]\n",
    "            if self.missing_values is not None:\n",
    "                x, n_obs = mean_impute(x, self.missing_values)  # Creates a copy\n",
    "            else:\n",
    "                n_obs = len(x)\n",
    "            if self.gaussianize == 'none':\n",
    "                pass\n",
    "            elif self.gaussianize == 'standard':\n",
    "                if fit:\n",
    "                    mean = np.mean(x, axis=0)\n",
    "                    # std = np.std(x, axis=0, ddof=0).clip(1e-10)\n",
    "                    std = np.sqrt(np.sum((x - mean)**2, axis=0) / n_obs).clip(1e-10)\n",
    "                    self.theta.append((mean, std))\n",
    "                x = ((x - self.theta[t][0]) / self.theta[t][1])\n",
    "                if np.max(np.abs(x)) > 6 and self.verbose:\n",
    "                    print(\"Warning: outliers more than 6 stds away from mean. Consider using gaussianize='outliers'\")\n",
    "            elif self.gaussianize == 'outliers':\n",
    "                if fit:\n",
    "                    mean = np.mean(x, axis=0)\n",
    "                    std = np.std(x, axis=0, ddof=0).clip(1e-10)\n",
    "                    self.theta.append((mean, std))\n",
    "                x = g((x - self.theta[t][0]) / self.theta[t][1])  # g truncates long tails\n",
    "            elif self.gaussianize == 'empirical':\n",
    "                print(\"Warning: correct inversion/transform of empirical gauss transform not implemented.\")\n",
    "                x = np.array([norm.ppf((rankdata(x_i) - 0.5) / len(x_i)) for x_i in x.T]).T\n",
    "            if self.gpu and fit:  # Don't return GPU matrices when only transforming\n",
    "                x = cm.CUDAMatrix(x)\n",
    "            X[t] = x\n",
    "        return X\n",
    "\n",
    "    def invert(self, x):\n",
    "        #$ NO CHANGE\n",
    "        \"\"\"Invert the preprocessing step to get x's in the original space.\"\"\"\n",
    "        if self.gaussianize == 'standard':\n",
    "            return self.theta[1] * x + self.theta[0]\n",
    "        elif self.gaussianize == 'outliers':\n",
    "            return self.theta[1] * g_inv(x) + self.theta[0]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def predict(self, y):\n",
    "        #$ DONE\n",
    "        #$ NOTE: not sure what does this function do\n",
    "        ret = [None] * self.nt\n",
    "        for t in range(self.nt):\n",
    "            ret[t] = self.invert(np.dot(self.moments[t][\"X_i Z_j\"], y[t].T).T)\n",
    "        return ret\n",
    "\n",
    "    def get_covariance(self):\n",
    "        #$ DONE\n",
    "        # This uses E(Xi|Y) formula for non-synergistic relationships\n",
    "        m = self.moments\n",
    "        ret = [None] * self.nt\n",
    "        for t in range(self.nt):\n",
    "            if self.discourage_overlap:\n",
    "                z = m[t]['rhoinvrho'] / (1 + m[t]['Si'])\n",
    "                cov = np.dot(z.T, z)\n",
    "                cov /= (1. - self.eps**2)\n",
    "                np.fill_diagonal(cov, 1)\n",
    "                ret[t] = self.theta[t][1][:, np.newaxis] * self.theta[t][1] * cov\n",
    "            else:\n",
    "                cov = np.einsum('ij,kj->ik', m[t][\"X_i Z_j\"], mp[t][\"X_i Y_j\"])\n",
    "                np.fill_diagonal(cov, 1)\n",
    "                ret[t] = self.theta[t][1][:, np.newaxis] * self.theta[t][1] * cov\n",
    "        return ret\n",
    "\n",
    "def pick_n_hidden(data, repeat=1, verbose=False):\n",
    "    #$ NO CHANGE\n",
    "    \"\"\"A helper function to pick the number of hidden factors / clusters to use.\"\"\"\n",
    "    # TODO: Use an efficient search strategy\n",
    "    max_score = - np.inf\n",
    "    n = 1\n",
    "    while True:\n",
    "        scores = []\n",
    "        for _ in range(repeat):\n",
    "            out = Corex(n_hidden=n, max_iter=1000, tol=1e-3, gpu=False).fit(data)\n",
    "            m = out.moments\n",
    "            scores.append(m[\"TC_no_overlap\"])\n",
    "        score = max(scores)\n",
    "        if verbose:\n",
    "            print(\"n: {}, score: {}\".format(n, score))\n",
    "        if score < max_score:\n",
    "            break\n",
    "        else:\n",
    "            n += 1\n",
    "            max_score = score\n",
    "    return n - 1\n",
    "\n",
    "\n",
    "def g(x, t=4):\n",
    "    #$ NO CHANGE\n",
    "    \"\"\"A transformation that suppresses outliers for a standard normal.\"\"\"\n",
    "    xp = np.clip(x, -t, t)\n",
    "    diff = np.tanh(x - xp)\n",
    "    return xp + diff\n",
    "\n",
    "\n",
    "def g_inv(x, t=4):\n",
    "    #$ NO CHANGE\n",
    "    \"\"\"Inverse of g transform.\"\"\"\n",
    "    xp = np.clip(x, -t, t)\n",
    "    diff = np.arctanh(np.clip(x - xp, -1 + 1e-10, 1 - 1e-10))\n",
    "    return xp + diff\n",
    "\n",
    "\n",
    "def mean_impute(x, v):\n",
    "    #$ NO CHANGE\n",
    "    \"\"\"Missing values in the data, x, are indicated by v. Wherever this value appears in x, it is replaced by the\n",
    "    mean value taken from the marginal distribution of that column.\"\"\"\n",
    "    if not np.isnan(v):\n",
    "        x = np.where(x == v, np.nan, x)\n",
    "    x_new = []\n",
    "    n_obs = []\n",
    "    for i, xi in enumerate(x.T):\n",
    "        missing_locs = np.where(np.isnan(xi))[0]\n",
    "        xi_nm = xi[np.isfinite(xi)]\n",
    "        xi[missing_locs] = np.mean(xi_nm)\n",
    "        x_new.append(xi)\n",
    "        n_obs.append(len(xi_nm))\n",
    "    return np.array(x_new).T, np.array(n_obs)\n",
    "\n",
    "\n",
    "def random_impute(x, v):\n",
    "    #$ NO CHANGE\n",
    "    \"\"\"Missing values in the data, x, are indicated by v. Wherever this value appears in x, it is replaced by a\n",
    "    random value taken from the marginal distribution of that column.\"\"\"\n",
    "    if not np.isnan(v):\n",
    "        x = np.where(x == v, np.nan, x)\n",
    "    x_new = []\n",
    "    for i, xi in enumerate(x.T):\n",
    "        missing_locs = np.where(np.isnan(xi))[0]\n",
    "        xi_nm = xi[np.isfinite(xi)]\n",
    "        xi[missing_locs] = np.random.choice(xi_nm, size=len(missing_locs))\n",
    "        x_new.append(xi)\n",
    "    return np.array(x_new).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data.shape = (887, 5038)\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    with open('../data/EOD_week.pkl', 'rb') as f:\n",
    "        df = pd.DataFrame(pkl.load(f))\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "print(\"Data.shape = {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starts = [0]\n",
    "ends = [x + 200 for x in starts]\n",
    "\n",
    "X = [df[s:e] for (s, e) in zip(starts, ends)]\n",
    "\n",
    "corex_params = {\n",
    "    'n_hidden':10,\n",
    "    'max_iter':300,\n",
    "    'tol':1e-5,\n",
    "    'anneal':True,\n",
    "    'missing_values':None,\n",
    "    'discourage_overlap':True,\n",
    "    'gaussianize':'standard',\n",
    "    'gpu':False,\n",
    "    'verbose':True,\n",
    "    'seed':None,\n",
    "    'optimizer': Adam()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CorEx with 10 latent factors\n",
      "Warning: outliers more than 6 stds away from mean. Consider using gaussianize='outliers'\n",
      "Warning: Convergence not achieved in 300 iterations. Final delta: 0.007076, Time: 15.3261501789\n",
      "Warning: Convergence not achieved in 300 iterations. Final delta: 0.039081, Time: 15.8301999569\n",
      "Warning: Convergence not achieved in 300 iterations. Final delta: 0.000328, Time: 15.38123703\n",
      "Warning: Convergence not achieved in 300 iterations. Final delta: 0.000530, Time: 15.043956995\n",
      "Warning: Convergence not achieved in 300 iterations. Final delta: 0.000616, Time: 13.1408040524\n",
      "Warning: Convergence not achieved in 300 iterations. Final delta: 0.000489, Time: 7.87024998665\n",
      "134 iterations to tol: 0.000010, Time: 2.98913598061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Corex at 0x7f8f7bdcf610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corex = Corex(**corex_params)\n",
    "corex.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[659.32140358630761]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corex.tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119216e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(corex.get_covariance()[0][:20, :20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118cd0fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(corex.get_covariance()[1][:20, :20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct `value` = $\\sigma(x^2) + cos(\\tau)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = corex.moments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_i^2 | Y (5038,)\n",
      "Y_j^2 (10,)\n",
      "TC_no_overlap ()\n",
      "I(X_i ; Y) (5038,)\n",
      "Qij (10, 5038)\n",
      "rhoinvrho (10, 5038)\n",
      "I(Y_j ; X) (10,)\n",
      "MI (10, 5038)\n",
      "ry (10, 10)\n",
      "X_i Y_j (5038, 10)\n",
      "invrho (10, 5038)\n",
      "Si (5038,)\n",
      "TCs (10,)\n",
      "rho (10, 5038)\n",
      "uj (10,)\n",
      "Qi (5038,)\n",
      "X_i Z_j (5038, 10)\n",
      "additivity ()\n",
      "TC_direct (10,)\n",
      "TC ()\n"
     ]
    }
   ],
   "source": [
    "for k,v in m.iteritems():\n",
    "    print k, np.shape(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
