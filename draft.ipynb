{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import vis_utils\n",
    "import misc_utils\n",
    "import metric_utils\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from scipy.stats import norm, rankdata\n",
    "import time\n",
    "import lasagne\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import cudamat as cm\n",
    "\n",
    "    GPU_SUPPORT = True\n",
    "except:\n",
    "    print(\"Install CUDA and cudamat (for python) to enable GPU speedups.\")\n",
    "    GPU_SUPPORT = False\n",
    "\n",
    "\n",
    "def g(x, t=4):\n",
    "    \"\"\"A transformation that suppresses outliers for a standard normal.\"\"\"\n",
    "    xp = np.clip(x, -t, t)\n",
    "    diff = np.tanh(x - xp)\n",
    "    return xp + diff\n",
    "\n",
    "\n",
    "def g_inv(x, t=4):\n",
    "    \"\"\"Inverse of g transform.\"\"\"\n",
    "    xp = np.clip(x, -t, t)\n",
    "    diff = np.arctanh(np.clip(x - xp, -1 + 1e-10, 1 - 1e-10))\n",
    "    return xp + diff\n",
    "\n",
    "\n",
    "def mean_impute(x, v):\n",
    "    \"\"\"Missing values in the data, x, are indicated by v. Wherever this value appears in x, it is replaced by the\n",
    "    mean value taken from the marginal distribution of that column.\"\"\"\n",
    "    if not np.isnan(v):\n",
    "        x = np.where(x == v, np.nan, x)\n",
    "    x_new = []\n",
    "    n_obs = []\n",
    "    for i, xi in enumerate(x.T):\n",
    "        missing_locs = np.where(np.isnan(xi))[0]\n",
    "        xi_nm = xi[np.isfinite(xi)]\n",
    "        xi[missing_locs] = np.mean(xi_nm)\n",
    "        x_new.append(xi)\n",
    "        n_obs.append(len(xi_nm))\n",
    "    return np.array(x_new).T, np.array(n_obs)\n",
    "\n",
    "\n",
    "class TimeCorex(object):\n",
    "    def __init__(self, nt, nv, n_hidden=10, max_iter=10000, tol=1e-5, anneal=True, missing_values=None,\n",
    "                 discourage_overlap=True, gaussianize='standard', gpu=False, y_scale=1.0, update_iter=10,\n",
    "                 verbose=False, seed=None):\n",
    "\n",
    "        self.nt = nt  # Number of timesteps\n",
    "        self.nv = nv  # Number of variables\n",
    "        self.m = n_hidden  # Number of latent factors to learn\n",
    "        self.max_iter = max_iter  # Number of iterations to try\n",
    "        self.tol = tol  # Threshold for convergence\n",
    "        self.anneal = anneal\n",
    "        self.eps = 0  # If anneal is True, it's adjusted during optimization to avoid local minima\n",
    "        self.missing_values = missing_values\n",
    "\n",
    "        self.discourage_overlap = discourage_overlap  # Whether or not to discourage overlapping latent factors\n",
    "        self.gaussianize = gaussianize  # Preprocess data: 'standard' scales to zero mean and unit variance\n",
    "        self.gpu = gpu  # Enable GPU support for some large matrix multiplications.\n",
    "        if self.gpu:\n",
    "            cm.cublas_init()\n",
    "\n",
    "        self.y_scale = y_scale  # Can be arbitrary, but sets the scale of Y\n",
    "        self.update_iter = update_iter  # Compute statistics every update_iter\n",
    "        np.random.seed(seed)  # Set seed for deterministic results\n",
    "        self.verbose = verbose\n",
    "        if verbose:\n",
    "            np.set_printoptions(precision=3, suppress=True, linewidth=160)\n",
    "            print('Linear CorEx with {:d} latent factors'.format(n_hidden))\n",
    "\n",
    "        self.history = [{} for t in range(self.nt)]  # Keep track of values for each iteration\n",
    "        self.rng = RandomStreams(seed)\n",
    "\n",
    "    def fit(self, x):\n",
    "        x = [np.array(xt, dtype=np.float32) for xt in x]\n",
    "        x = self.preprocess(x, fit=True)  # Fit a transform for each marginal\n",
    "        self.x_std = x  # to have an access to standardalized x\n",
    "\n",
    "        anneal_schedule = [0.]\n",
    "        if self.anneal:\n",
    "            anneal_schedule = [0.6 ** k for k in range(1, 7)] + [0]\n",
    "\n",
    "        self._define_model()\n",
    "\n",
    "        for i_eps, eps in enumerate(anneal_schedule):\n",
    "            start_time = time.time()\n",
    "\n",
    "            self.eps = eps\n",
    "            self.anneal_eps.set_value(np.float32(eps))\n",
    "            self.moments = self._calculate_moments(x, self.ws, quick=True)\n",
    "            self._update_u(x)\n",
    "\n",
    "            for i_loop in range(self.max_iter):\n",
    "                ret = self.train_step(*x)\n",
    "                obj = ret[0]\n",
    "                reg_obj = ret[2]\n",
    "\n",
    "                if i_loop % self.update_iter == 0 and self.verbose:\n",
    "                    self.moments = self._calculate_moments(x, self.ws, quick=True)\n",
    "                    self._update_u(x)\n",
    "                    print(\"tc = {}, obj = {}, reg = {}, eps = {}\".format(np.sum(self.tc),\n",
    "                                                                         obj, reg_obj, eps))\n",
    "\n",
    "            print(\"Annealing iteration finished, time = {}\".format(time.time() - start_time))\n",
    "\n",
    "        self.moments = self._calculate_moments(x, self.ws, quick=False)  # Update moments with details\n",
    "        return self\n",
    "\n",
    "    def _update_u(self, x):\n",
    "        self.us = [self.getus(w.get_value(), xt) for w, xt in zip(self.ws, x)]\n",
    "\n",
    "    def update_records(self, moments, delta):\n",
    "        \"\"\"Print and store some statistics about each iteration.\"\"\"\n",
    "        gc.disable()  # There's a bug that slows when appending, fixed by temporarily disabling garbage collection\n",
    "        for t in range(self.nt):\n",
    "            self.history[t][\"TC\"] = self.history[t].get(\"TC\", []) + [moments[t][\"TC\"]]\n",
    "        if self.verbose > 1:\n",
    "            tc_sum = sum([m[\"TC\"] for m in moments])\n",
    "            add_sum = sum([m.get(\"additivity\", 0) for m in moments])\n",
    "            print(\"TC={:.3f}\\tadd={:.3f}\\tdelta={:.6f}\".format(tc_sum, add_sum, delta))\n",
    "        if self.verbose:\n",
    "            for t in range(self.nt):\n",
    "                self.history[t][\"additivity\"] = self.history[t].get(\"additivity\", []) + [\n",
    "                    moments[t].get(\"additivity\", 0)]\n",
    "                self.history[t][\"TCs\"] = self.history[t].get(\"TCs\", []) + [moments[t].get(\"TCs\", np.zeros(self.m))]\n",
    "        gc.enable()\n",
    "\n",
    "    @property\n",
    "    def tc(self):\n",
    "        \"\"\"This actually returns the lower bound on TC that is optimized. The lower bound assumes a constraint that\n",
    "         would be satisfied by a non-overlapping model.\n",
    "         Check \"moments\" for two other estimates of TC that may be useful.\"\"\"\n",
    "        return [m[\"TC\"] for m in self.moments]\n",
    "\n",
    "    @property\n",
    "    def tcs(self):\n",
    "        \"\"\"TCs for each individual latent factor. They DO NOT sum to TC in this case, because of overlaps.\"\"\"\n",
    "        return [m[\"TCs\"] for m in self.moments]\n",
    "\n",
    "    @property\n",
    "    def mis(self):\n",
    "        return [-0.5 * np.log1p(-m[\"rho\"] ** 2) for m in self.moments]\n",
    "\n",
    "    def clusters(self):\n",
    "        return np.argmax(np.abs(self.us), axis=0)  # TODO: understand this\n",
    "\n",
    "    def _sig(self, x, u):\n",
    "        \"\"\"Multiple the matrix u by the covariance matrix of x. We are interested in situations where\n",
    "        n_variables >> n_samples, so we do this without explicitly constructing the covariance matrix.\"\"\"\n",
    "        n_samples = x.shape[0]\n",
    "        if self.gpu:\n",
    "            y = cm.empty((n_samples, self.m))\n",
    "            uc = cm.CUDAMatrix(u)\n",
    "            cm.dot(x, uc.T, target=y)\n",
    "            del uc\n",
    "            tmp = cm.empty((self.nv, self.m))\n",
    "            cm.dot(x.T, y, target=tmp)\n",
    "            tmp_dot = tmp.asarray()\n",
    "            del y\n",
    "            del tmp\n",
    "        else:\n",
    "            y = x.dot(u.T)\n",
    "            tmp_dot = x.T.dot(y)\n",
    "        prod = (1 - self.eps ** 2) * tmp_dot.T / n_samples + self.eps ** 2 * u  # nv by m,  <X_i Y_j> / std Y_j\n",
    "        return prod\n",
    "\n",
    "    def getus(self, w, x):\n",
    "        # U_{ji} = \\frac{W_{ji}}{\\sqrt{E[Z_j^2]}}\n",
    "        \"\"\"\n",
    "        U = np.zeros(W.shape)\n",
    "        for j in range(W.shape[0]):\n",
    "            U[j, :] = W[j, :] / np.sqrt(self.z2_fromW(j, W, x))\n",
    "        return U\n",
    "        \"\"\"\n",
    "        tmp_dot = np.dot(self._sig(x, w), w.T)\n",
    "        z2 = self.y_scale ** 2 + np.einsum(\"ii->i\", tmp_dot)\n",
    "        return w / np.sqrt(z2).reshape((-1, 1))\n",
    "\n",
    "    def _calculate_moments(self, x, ws, quick=False):\n",
    "        us = [self.getus(w.get_value(), xt) for w, xt in zip(ws, x)]\n",
    "        ret = [None] * self.nt\n",
    "        for t in range(self.nt):\n",
    "            if self.discourage_overlap:\n",
    "                ret[t] = self._calculate_moments_ns(x[t], us[t], quick=quick)\n",
    "            else:\n",
    "                ret[t] = self._calculate_moments_syn(x[t], us[t], quick=quick)\n",
    "        return ret\n",
    "\n",
    "    def _calculate_moments_ns(self, x, ws, quick=False):\n",
    "        \"\"\"Calculate moments based on the weights and samples. We also calculate and save MI, TC, additivity, and\n",
    "        the value of the objective. Note it is assumed that <X_i^2> = 1! \"\"\"\n",
    "        m = {}  # Dictionary of moments\n",
    "        n_samples = x.shape[0]\n",
    "        if self.gpu:\n",
    "            y = cm.empty((n_samples, self.m))\n",
    "            wc = cm.CUDAMatrix(ws)\n",
    "            cm.dot(x, wc.T, target=y)  # + noise, but it is included analytically\n",
    "            del wc\n",
    "            tmp_sum = np.einsum('lj,lj->j', y.asarray(), y.asarray())  # TODO: Should be able to do on gpu...\n",
    "        else:\n",
    "            y = x.dot(ws.T)\n",
    "            tmp_sum = np.einsum('lj,lj->j', y, y)\n",
    "        m[\"uj\"] = (1 - self.eps ** 2) * tmp_sum / n_samples + self.eps ** 2 * np.sum(ws ** 2, axis=1)\n",
    "\n",
    "        if self.gpu:\n",
    "            tmp = cm.empty((self.nv, self.m))\n",
    "            cm.dot(x.T, y, target=tmp)\n",
    "            tmp_dot = tmp.asarray()\n",
    "            del tmp\n",
    "            del y\n",
    "        else:\n",
    "            tmp_dot = x.T.dot(y)\n",
    "        m[\"rho\"] = (1 - self.eps ** 2) * tmp_dot.T / n_samples + self.eps ** 2 * ws  # m by nv\n",
    "        m[\"ry\"] = ws.dot(m[\"rho\"].T)  # normalized covariance of Y\n",
    "        m[\"Y_j^2\"] = self.y_scale ** 2 / (1. - m[\"uj\"])\n",
    "        np.fill_diagonal(m[\"ry\"], 1)\n",
    "        m[\"invrho\"] = 1. / (1. - m[\"rho\"] ** 2)\n",
    "        m[\"rhoinvrho\"] = m[\"rho\"] * m[\"invrho\"]\n",
    "        m[\"Qij\"] = np.dot(m['ry'], m[\"rhoinvrho\"])\n",
    "        m[\"Qi\"] = np.einsum('ki,ki->i', m[\"rhoinvrho\"], m[\"Qij\"])\n",
    "        # m[\"Qi-Si^2\"] = np.einsum('ki,ki->i', m[\"rhoinvrho\"], m[\"Qij\"])\n",
    "        m[\"Si\"] = np.sum(m[\"rho\"] * m[\"rhoinvrho\"], axis=0)\n",
    "\n",
    "        # This is the objective, a lower bound for TC\n",
    "        m[\"TC\"] = np.sum(np.log(1 + m[\"Si\"])) \\\n",
    "                  - 0.5 * np.sum(np.log(1 - m[\"Si\"] ** 2 + m[\"Qi\"])) \\\n",
    "                  + 0.5 * np.sum(np.log(1 - m[\"uj\"]))\n",
    "\n",
    "        if not quick:\n",
    "            m[\"MI\"] = - 0.5 * np.log1p(-m[\"rho\"] ** 2)\n",
    "            m[\"X_i Y_j\"] = m[\"rho\"].T * np.sqrt(m[\"Y_j^2\"])\n",
    "            m[\"X_i Z_j\"] = np.linalg.solve(m[\"ry\"], m[\"rho\"]).T\n",
    "            m[\"X_i^2 | Y\"] = (1. - np.einsum('ij,ji->i', m[\"X_i Z_j\"], m[\"rho\"])).clip(1e-6)\n",
    "            m['I(Y_j ; X)'] = 0.5 * np.log(m[\"Y_j^2\"]) - 0.5 * np.log(self.y_scale ** 2)\n",
    "            m['I(X_i ; Y)'] = - 0.5 * np.log(m[\"X_i^2 | Y\"])\n",
    "            m[\"TCs\"] = m[\"MI\"].sum(axis=1) - m['I(Y_j ; X)']\n",
    "            m[\"TC_no_overlap\"] = m[\"MI\"].max(axis=0).sum() - m[\n",
    "                'I(Y_j ; X)'].sum()  # A direct calculation of TC where each variable is in exactly one group.\n",
    "            m[\"TC_direct\"] = m['I(X_i ; Y)'].sum() - m[\n",
    "                'I(Y_j ; X)']  # A direct calculation of TC. Should be upper bound for \"TC\", \"TC_no_overlap\"\n",
    "            m[\"additivity\"] = (m[\"MI\"].sum(axis=0) - m['I(X_i ; Y)']).sum()\n",
    "        return m\n",
    "\n",
    "    def _calculate_moments_syn(self, x, ws, quick=False):\n",
    "        return None\n",
    "\n",
    "    def transform(self, x, details=False):\n",
    "        \"\"\"Transform an array of inputs, x, into an array of k latent factors, Y.\n",
    "            Optionally, you can get the remainder information and/or stop at a specified level.\"\"\"\n",
    "        x = self.preprocess(x)\n",
    "        ret = [a.dot(w.get_value().T) for (a, w) in zip(x, self.ws)]\n",
    "        if details:\n",
    "            moments = self._calculate_moments(x, self.us)\n",
    "            return ret, moments\n",
    "        return ret\n",
    "\n",
    "    def preprocess(self, X, fit=False):\n",
    "        \"\"\"Transform each marginal to be as close to a standard Gaussian as possible.\n",
    "        'standard' (default) just subtracts the mean and scales by the std.\n",
    "        'empirical' does an empirical gaussianization (but this cannot be inverted).\n",
    "        'outliers' tries to squeeze in the outliers\n",
    "        Any other choice will skip the transformation.\"\"\"\n",
    "        ret = [None] * len(X)\n",
    "        if fit:\n",
    "            self.theta = []\n",
    "        for t in range(len(X)):\n",
    "            x = X[t]\n",
    "            if self.missing_values is not None:\n",
    "                x, n_obs = mean_impute(x, self.missing_values)  # Creates a copy\n",
    "            else:\n",
    "                n_obs = len(x)\n",
    "            if self.gaussianize == 'none':\n",
    "                pass\n",
    "            elif self.gaussianize == 'standard':\n",
    "                if fit:\n",
    "                    mean = np.mean(x, axis=0)\n",
    "                    # std = np.std(x, axis=0, ddof=0).clip(1e-10)\n",
    "                    std = np.sqrt(np.sum((x - mean) ** 2, axis=0) / n_obs).clip(1e-10)\n",
    "                    self.theta.append((mean, std))\n",
    "                x = ((x - self.theta[t][0]) / self.theta[t][1])\n",
    "                if np.max(np.abs(x)) > 6 and self.verbose:\n",
    "                    print(\"Warning: outliers more than 6 stds away from mean. Consider using gaussianize='outliers'\")\n",
    "            elif self.gaussianize == 'outliers':\n",
    "                if fit:\n",
    "                    mean = np.mean(x, axis=0)\n",
    "                    std = np.std(x, axis=0, ddof=0).clip(1e-10)\n",
    "                    self.theta.append((mean, std))\n",
    "                x = g((x - self.theta[t][0]) / self.theta[t][1])  # g truncates long tails\n",
    "            elif self.gaussianize == 'empirical':\n",
    "                print(\"Warning: correct inversion/transform of empirical gauss transform not implemented.\")\n",
    "                x = np.array([norm.ppf((rankdata(x_i) - 0.5) / len(x_i)) for x_i in x.T]).T\n",
    "            if self.gpu and fit:  # Don't return GPU matrices when only transforming\n",
    "                x = cm.CUDAMatrix(x)\n",
    "            ret[t] = x\n",
    "        return ret\n",
    "\n",
    "    def invert(self, x):\n",
    "        # TODO: consider timesteps\n",
    "        \"\"\"Invert the preprocessing step to get x's in the original space.\"\"\"\n",
    "        if self.gaussianize == 'standard':\n",
    "            return self.theta[1] * x + self.theta[0]\n",
    "        elif self.gaussianize == 'outliers':\n",
    "            return self.theta[1] * g_inv(x) + self.theta[0]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def predict(self, y):\n",
    "        # NOTE: not sure what does this function do\n",
    "        ret = [None] * self.nt\n",
    "        for t in range(self.nt):\n",
    "            ret[t] = self.invert(np.dot(self.moments[t][\"X_i Z_j\"], y[t].T).T)\n",
    "        return ret\n",
    "\n",
    "    def get_covariance(self):\n",
    "        # This uses E(Xi|Y) formula for non-synergistic relationships\n",
    "        m = self.moments\n",
    "        ret = [None] * self.nt\n",
    "        for t in range(self.nt):\n",
    "            if self.discourage_overlap:\n",
    "                z = m[t]['rhoinvrho'] / (1 + m[t]['Si'])\n",
    "                cov = np.dot(z.T, z)\n",
    "                cov /= (1. - self.eps ** 2)\n",
    "                np.fill_diagonal(cov, 1)\n",
    "                ret[t] = self.theta[t][1][:, np.newaxis] * self.theta[t][1] * cov\n",
    "            else:\n",
    "                cov = np.einsum('ij,kj->ik', m[t][\"X_i Z_j\"], mp[t][\"X_i Y_j\"])\n",
    "                np.fill_diagonal(cov, 1)\n",
    "                ret[t] = self.theta[t][1][:, np.newaxis] * self.theta[t][1] * cov\n",
    "        return ret\n",
    "\n",
    "\n",
    "class TimeCorexW(TimeCorex):\n",
    "    def __init__(self, l1=0.0, l2=0.0, **kwargs):\n",
    "        super(TimeCorexW, self).__init__(**kwargs)\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def _define_model(self):\n",
    "\n",
    "        self.x_wno = [None] * self.nt\n",
    "        self.x = [None] * self.nt\n",
    "        self.ws = [None] * self.nt\n",
    "        self.z_mean = [None] * self.nt\n",
    "        self.z = [None] * self.nt\n",
    "\n",
    "        self.anneal_eps = theano.shared(np.float32(0))\n",
    "\n",
    "        for t in range(self.nt):\n",
    "            self.x_wno[t] = T.matrix('X')\n",
    "            ns = self.x_wno[t].shape[0]\n",
    "            anneal_noise = self.rng.normal(size=(ns, self.nv))\n",
    "            self.x[t] = np.sqrt(1 - self.anneal_eps ** 2) * self.x_wno[t] + self.anneal_eps * anneal_noise\n",
    "            z_noise = self.rng.normal(avg=0.0, std=self.y_scale, size=(ns, self.m))\n",
    "            self.ws[t] = theano.shared(1.0 / np.sqrt(self.nv) * np.random.randn(self.m, self.nv), name='W{}'.format(t))\n",
    "            self.z_mean[t] = T.dot(self.x[t], self.ws[t].T)\n",
    "            self.z[t] = self.z_mean[t] + z_noise\n",
    "\n",
    "        EPS = 1e-5\n",
    "        self.objs = [None] * self.nt\n",
    "        self.sigma = [None] * self.nt\n",
    "\n",
    "        for t in range(self.nt):\n",
    "            z2 = (self.z[t] ** 2).mean(axis=0)  # (m,)\n",
    "            ns = self.x_wno[t].shape[0]\n",
    "            R = T.dot(self.z[t].T, self.x[t]) / ns  # m, nv\n",
    "            R = R / T.sqrt(z2).reshape((self.m, 1))  # as <x^2_i> == 1 we don't divide by it\n",
    "            ri = ((R ** 2) / T.clip(1 - R ** 2, EPS, 1 - EPS)).sum(axis=0)  # (nv,)\n",
    "\n",
    "            # v_xi | z conditional mean\n",
    "            outer_term = (1 / (1 + ri)).reshape((1, self.nv))\n",
    "            inner_term_1 = (R / T.clip(1 - R ** 2, EPS, 1) / T.sqrt(z2).reshape((self.m, 1))).reshape(\n",
    "                (1, self.m, self.nv))\n",
    "            inner_term_2 = self.z[t].reshape((ns, self.m, 1))\n",
    "            cond_mean = outer_term * ((inner_term_1 * inner_term_2).sum(axis=1))  # (ns, nv)\n",
    "\n",
    "            inner_mat = 1.0 / (1 + ri).reshape((1, self.nv)) * R / T.clip(1 - R ** 2, EPS, 1)\n",
    "            self.sigma[t] = T.dot(inner_mat.T, inner_mat)\n",
    "            self.sigma[t] = self.sigma[t] * (1 - T.eye(self.nv)) + T.eye(self.nv)\n",
    "\n",
    "            # objective\n",
    "            obj_part_1 = 0.5 * T.log(T.clip(((self.x[t] - cond_mean) ** 2).mean(axis=0), EPS, np.inf)).sum(axis=0)\n",
    "            obj_part_2 = 0.5 * T.log(z2).sum(axis=0)\n",
    "            self.objs[t] = obj_part_1 + obj_part_2\n",
    "\n",
    "        self.main_obj = T.sum(self.objs)\n",
    "\n",
    "        # regularization\n",
    "        self.reg_obj = T.constant(0)\n",
    "\n",
    "        if self.l1 > 0:\n",
    "            l1_reg = T.sum([T.abs_(self.ws[t + 1] - self.ws[t]).sum() for t in range(self.nt - 1)])\n",
    "            self.reg_obj = self.reg_obj + self.l1 * l1_reg\n",
    "\n",
    "        if self.l2 > 0:\n",
    "            l2_reg = T.sum([T.square(self.ws[t + 1] - self.ws[t]).sum() for t in range(self.nt - 1)])\n",
    "            self.reg_obj = self.reg_obj + self.l2 * l2_reg\n",
    "\n",
    "        self.total_obj = self.main_obj + self.reg_obj\n",
    "\n",
    "        # optimizer\n",
    "        updates = lasagne.updates.adam(self.total_obj, self.ws)\n",
    "\n",
    "        # functions\n",
    "        self.train_step = theano.function(inputs=self.x_wno,\n",
    "                                          outputs=[self.total_obj, self.main_obj, self.reg_obj] + self.objs,\n",
    "                                          updates=updates)\n",
    "\n",
    "        self.get_norm_covariance = theano.function(inputs=self.x_wno,\n",
    "                                                   outputs=self.sigma)\n",
    "\n",
    "\n",
    "class TimeCorexWWT(TimeCorex):\n",
    "    def __init__(self, l1=0.0, l2=0.0, **kwargs):\n",
    "        super(TimeCorexWWT, self).__init__(**kwargs)\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def _define_model(self):\n",
    "\n",
    "        self.x_wno = [None] * self.nt\n",
    "        self.x = [None] * self.nt\n",
    "        self.ws = [None] * self.nt\n",
    "        self.z_mean = [None] * self.nt\n",
    "        self.z = [None] * self.nt\n",
    "\n",
    "        self.anneal_eps = theano.shared(np.float32(0))\n",
    "\n",
    "        for t in range(self.nt):\n",
    "            self.x_wno[t] = T.matrix('X')\n",
    "            ns = self.x_wno[t].shape[0]\n",
    "            anneal_noise = self.rng.normal(size=(ns, self.nv))\n",
    "            self.x[t] = np.sqrt(1 - self.anneal_eps ** 2) * self.x_wno[t] + self.anneal_eps * anneal_noise\n",
    "            z_noise = self.rng.normal(avg=0.0, std=self.y_scale, size=(ns, self.m))\n",
    "            self.ws[t] = theano.shared(1.0 / np.sqrt(self.nv) * np.random.randn(self.m, self.nv), name='W{}'.format(t))\n",
    "            self.z_mean[t] = T.dot(self.x[t], self.ws[t].T)\n",
    "            self.z[t] = self.z_mean[t] + z_noise\n",
    "\n",
    "        EPS = 1e-5\n",
    "        self.objs = [None] * self.nt\n",
    "        self.sigma = [None] * self.nt\n",
    "\n",
    "        for t in range(self.nt):\n",
    "            z2 = (self.z[t] ** 2).mean(axis=0)  # (m,)\n",
    "            ns = self.x_wno[t].shape[0]\n",
    "            R = T.dot(self.z[t].T, self.x[t]) / ns  # m, nv\n",
    "            R = R / T.sqrt(z2).reshape((self.m, 1))  # as <x^2_i> == 1 we don't divide by it\n",
    "            ri = ((R ** 2) / T.clip(1 - R ** 2, EPS, 1 - EPS)).sum(axis=0)  # (nv,)\n",
    "\n",
    "            # v_xi | z conditional mean\n",
    "            outer_term = (1 / (1 + ri)).reshape((1, self.nv))\n",
    "            inner_term_1 = (R / T.clip(1 - R ** 2, EPS, 1) / T.sqrt(z2).reshape((self.m, 1))).reshape(\n",
    "                (1, self.m, self.nv))\n",
    "            inner_term_2 = self.z[t].reshape((ns, self.m, 1))\n",
    "            cond_mean = outer_term * ((inner_term_1 * inner_term_2).sum(axis=1))  # (ns, nv)\n",
    "\n",
    "            inner_mat = 1.0 / (1 + ri).reshape((1, self.nv)) * R / T.clip(1 - R ** 2, EPS, 1)\n",
    "            self.sigma[t] = T.dot(inner_mat.T, inner_mat)\n",
    "            self.sigma[t] = self.sigma[t] * (1 - T.eye(self.nv)) + T.eye(self.nv)\n",
    "\n",
    "            # objective\n",
    "            obj_part_1 = 0.5 * T.log(T.clip(((self.x[t] - cond_mean) ** 2).mean(axis=0), EPS, np.inf)).sum(axis=0)\n",
    "            obj_part_2 = 0.5 * T.log(z2).sum(axis=0)\n",
    "            self.objs[t] = obj_part_1 + obj_part_2\n",
    "\n",
    "        self.main_obj = T.sum(self.objs)\n",
    "\n",
    "        # regularization\n",
    "        self.reg_obj = T.constant(0)\n",
    "\n",
    "        self.S = [None] * self.nt\n",
    "        for t in range(self.nt):\n",
    "            self.S[t] = T.dot(self.ws[t].T, self.ws[t])\n",
    "\n",
    "        if self.l1 > 0:\n",
    "            l1_reg = T.sum([T.abs_(self.S[t + 1] - self.S[t]).sum() for t in range(self.nt - 1)])\n",
    "            self.reg_obj = self.reg_obj + self.l1 * l1_reg\n",
    "\n",
    "        if self.l2 > 0:\n",
    "            l2_reg = T.sum([T.square(self.S[t + 1] - self.S[t]).sum() for t in range(self.nt - 1)])\n",
    "            self.reg_obj = self.reg_obj + self.l2 * l2_reg\n",
    "\n",
    "        self.total_obj = self.main_obj + self.reg_obj\n",
    "\n",
    "        # optimizer\n",
    "        updates = lasagne.updates.adam(self.total_obj, self.ws)\n",
    "\n",
    "        # functions\n",
    "        self.train_step = theano.function(inputs=self.x_wno,\n",
    "                                          outputs=[self.total_obj, self.main_obj, self.reg_obj] + self.objs,\n",
    "                                          updates=updates)\n",
    "\n",
    "        self.get_norm_covariance = theano.function(inputs=self.x_wno,\n",
    "                                                   outputs=self.sigma)\n",
    "\n",
    "\n",
    "class TimeCorexGlobalMI(TimeCorex):\n",
    "    def __init__(self, l1=0.0, l2=0.0, **kwargs):\n",
    "        super(TimeCorexGlobalMI, self).__init__(**kwargs)\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def _define_model(self):\n",
    "\n",
    "        self.x_wno = [None] * self.nt\n",
    "        self.x = [None] * self.nt\n",
    "        self.ws = [None] * self.nt\n",
    "        self.z_mean = [None] * self.nt\n",
    "        self.z = [None] * self.nt\n",
    "\n",
    "        self.anneal_eps = theano.shared(np.float32(0))\n",
    "\n",
    "        for t in range(self.nt):\n",
    "            self.x_wno[t] = T.matrix('X')\n",
    "            ns = self.x_wno[t].shape[0]\n",
    "            anneal_noise = self.rng.normal(size=(ns, self.nv))\n",
    "            self.x[t] = np.sqrt(1 - self.anneal_eps ** 2) * self.x_wno[t] + self.anneal_eps * anneal_noise\n",
    "            z_noise = self.rng.normal(avg=0.0, std=self.y_scale, size=(ns, self.m))\n",
    "            self.ws[t] = theano.shared(1.0 / np.sqrt(self.nv) * np.random.randn(self.m, self.nv), name='W{}'.format(t))\n",
    "            self.z_mean[t] = T.dot(self.x[t], self.ws[t].T)\n",
    "            self.z[t] = self.z_mean[t] + z_noise\n",
    "\n",
    "        EPS = 1e-5\n",
    "        self.objs = [None] * self.nt\n",
    "        self.mizx = [None] * self.nt\n",
    "\n",
    "        for t in range(self.nt):\n",
    "            z2 = (self.z[t] ** 2).mean(axis=0)  # (m,)\n",
    "            ns = self.x_wno[t].shape[0]\n",
    "            R = T.dot(self.z[t].T, self.x[t]) / ns  # m, nv\n",
    "            R = R / T.sqrt(z2).reshape((self.m, 1))  # as <x^2_i> == 1 we don't divide by it\n",
    "            ri = ((R ** 2) / T.clip(1 - R ** 2, EPS, 1 - EPS)).sum(axis=0)  # (nv,)\n",
    "\n",
    "            self.mizx[t] = -0.5 * T.log1p(-T.clip(R ** 2, 0, 1 - EPS))\n",
    "\n",
    "            # v_xi | z conditional mean\n",
    "            outer_term = (1 / (1 + ri)).reshape((1, self.nv))\n",
    "            inner_term_1 = (R / T.clip(1 - R ** 2, EPS, 1) / T.sqrt(z2).reshape((self.m, 1))).reshape(\n",
    "                (1, self.m, self.nv))\n",
    "            inner_term_2 = self.z[t].reshape((ns, self.m, 1))\n",
    "            cond_mean = outer_term * ((inner_term_1 * inner_term_2).sum(axis=1))  # (ns, nv)\n",
    "\n",
    "            # objective\n",
    "            obj_part_1 = 0.5 * T.log(T.clip(((self.x[t] - cond_mean) ** 2).mean(axis=0), EPS, np.inf)).sum(axis=0)\n",
    "            obj_part_2 = 0.5 * T.log(z2).sum(axis=0)\n",
    "            self.objs[t] = obj_part_1 + obj_part_2\n",
    "\n",
    "        self.main_obj = T.sum(self.objs)\n",
    "\n",
    "        # regularization\n",
    "        self.reg_obj = T.constant(0)\n",
    "\n",
    "        if self.l1 > 0:\n",
    "            l1_reg = T.sum([T.abs_(self.mizx[t + 1] - self.mizx[t]).sum() for t in range(self.nt - 1)])\n",
    "            self.reg_obj = self.reg_obj + self.l1 * l1_reg\n",
    "\n",
    "        if self.l2 > 0:\n",
    "            l2_reg = T.sum([T.square(self.mizx[t + 1] - self.mizx[t]).sum() for t in range(self.nt - 1)])\n",
    "            self.reg_obj = self.reg_obj + self.l2 * l2_reg\n",
    "\n",
    "        self.total_obj = self.main_obj + self.reg_obj\n",
    "\n",
    "        # optimizer\n",
    "        updates = lasagne.updates.adam(self.total_obj, self.ws)\n",
    "\n",
    "        # functions\n",
    "        self.train_step = theano.function(inputs=self.x_wno,\n",
    "                                          outputs=[self.total_obj, self.main_obj, self.reg_obj] + self.objs,\n",
    "                                          updates=updates)\n",
    "\n",
    "\n",
    "class TimeCorexSigma(TimeCorex):\n",
    "    def __init__(self, l1=0.0, l2=0.0, **kwargs):\n",
    "        super(TimeCorexSigma, self).__init__(**kwargs)\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def _define_model(self):\n",
    "\n",
    "        self.x_wno = [None] * self.nt\n",
    "        self.x = [None] * self.nt\n",
    "        self.ws = [None] * self.nt\n",
    "        self.z_mean = [None] * self.nt\n",
    "        self.z = [None] * self.nt\n",
    "\n",
    "        self.anneal_eps = theano.shared(np.float32(0))\n",
    "\n",
    "        for t in range(self.nt):\n",
    "            self.x_wno[t] = T.matrix('X')\n",
    "            ns = self.x_wno[t].shape[0]\n",
    "            anneal_noise = self.rng.normal(size=(ns, self.nv))\n",
    "            self.x[t] = np.sqrt(1 - self.anneal_eps ** 2) * self.x_wno[t] + self.anneal_eps * anneal_noise\n",
    "            z_noise = self.rng.normal(avg=0.0, std=self.y_scale, size=(ns, self.m))\n",
    "            self.ws[t] = theano.shared(1.0 / np.sqrt(self.nv) * np.random.randn(self.m, self.nv), name='W{}'.format(t))\n",
    "            self.z_mean[t] = T.dot(self.x[t], self.ws[t].T)\n",
    "            self.z[t] = self.z_mean[t] + z_noise\n",
    "\n",
    "        EPS = 1e-5\n",
    "        self.objs = [None] * self.nt\n",
    "        self.sigma = [None] * self.nt\n",
    "        self.R = [None] * self.nt\n",
    "\n",
    "        for t in range(self.nt):\n",
    "            z2 = (self.z[t] ** 2).mean(axis=0)  # (m,)\n",
    "            ns = self.x_wno[t].shape[0]\n",
    "            R = T.dot(self.z[t].T, self.x[t]) / ns  # m, nv\n",
    "            R = R / T.sqrt(z2).reshape((self.m, 1))  # as <x^2_i> == 1 we don't divide by it\n",
    "            ri = ((R ** 2) / T.clip(1 - R ** 2, EPS, 1 - EPS)).sum(axis=0)  # (nv,)\n",
    "\n",
    "            # v_xi | z conditional mean\n",
    "            outer_term = (1 / (1 + ri)).reshape((1, self.nv))\n",
    "            inner_term_1 = (R / T.clip(1 - R ** 2, EPS, 1) / T.sqrt(z2).reshape((self.m, 1))).reshape(\n",
    "                (1, self.m, self.nv))\n",
    "            inner_term_2 = self.z[t].reshape((ns, self.m, 1))\n",
    "            cond_mean = outer_term * ((inner_term_1 * inner_term_2).sum(axis=1))  # (ns, nv)\n",
    "\n",
    "            inner_mat = 1.0 / (1 + ri).reshape((1, self.nv)) * R / T.clip(1 - R ** 2, EPS, 1)\n",
    "            self.sigma[t] = T.dot(inner_mat.T, inner_mat)\n",
    "            self.sigma[t] = self.sigma[t] * (1 - T.eye(self.nv)) + T.eye(self.nv)\n",
    "\n",
    "            self.R[t] = R\n",
    "\n",
    "            # objective\n",
    "            obj_part_1 = 0.5 * T.log(T.clip(((self.x[t] - cond_mean) ** 2).mean(axis=0), EPS, np.inf)).sum(axis=0)\n",
    "            obj_part_2 = 0.5 * T.log(z2).sum(axis=0)\n",
    "            self.objs[t] = obj_part_1 + obj_part_2\n",
    "\n",
    "        self.main_obj = T.sum(self.objs)\n",
    "\n",
    "        # regularization\n",
    "        self.reg_obj = T.constant(0)\n",
    "\n",
    "        if self.l1 > 0:\n",
    "            l1_reg = T.sum([T.abs_(self.sigma[t + 1] - self.sigma[t]).sum() for t in range(self.nt - 1)])\n",
    "            self.reg_obj = self.reg_obj + self.l1 * l1_reg\n",
    "\n",
    "        if self.l2 > 0:\n",
    "            l2_reg = T.sum([T.square(self.sigma[t + 1] - self.sigma[t]).sum() for t in range(self.nt - 1)])\n",
    "            self.reg_obj = self.reg_obj + self.l2 * l2_reg\n",
    "\n",
    "        self.total_obj = self.main_obj + self.reg_obj\n",
    "\n",
    "        # optimizer\n",
    "        updates = lasagne.updates.adam(self.total_obj, self.ws)\n",
    "\n",
    "        # functions\n",
    "        self.train_step = theano.function(inputs=self.x_wno,\n",
    "                                          outputs=[self.total_obj, self.main_obj, self.reg_obj] + self.objs,\n",
    "                                          updates=updates)\n",
    "\n",
    "        self.get_norm_covariance = theano.function(inputs=self.x_wno,\n",
    "                                                   outputs=self.sigma)\n",
    "\n",
    "        self.get_R = theano.function(inputs=self.x_wno,\n",
    "                                     outputs=self.R)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
