{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \n",
    "    \"\"\"Adam optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        \n",
    "        allowed_kwargs = {'clipnorm', 'clipvalue'}\n",
    "        for k in kwargs:\n",
    "            if k not in allowed_kwargs:\n",
    "                raise TypeError('Unexpected keyword argument '\n",
    "                                'passed to optimizer: ' + str(k))\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.iterations = 0\n",
    "        self.lr = lr\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    def get_update(self, params, grads):\n",
    "        \"\"\" params and grads are list of numpy arrays\n",
    "        \"\"\"\n",
    "        original_shapes = [x.shape for x in params]\n",
    "        params = [x.flatten() for x in params]\n",
    "        grads = [x.flatten() for x in grads]\n",
    "        \n",
    "        \"\"\" #TODO: implement clipping\n",
    "        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n",
    "            norm = np.sqrt(sum([np.sum(np.square(g)) for g in grads]))\n",
    "            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n",
    "        if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n",
    "            grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n",
    "        \"\"\"\n",
    "        \n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "        lr_t = lr * (np.sqrt(1. - np.power(self.beta_2, t)) /\n",
    "                     (1. - np.power(self.beta_1, t)))\n",
    "\n",
    "        if not hasattr(self, 'ms'):\n",
    "            self.ms = [np.zeros(p.shape) for p in params]\n",
    "            self.vs = [np.zeros(p.shape) for p in params]\n",
    "    \n",
    "        ret = [None] * len(params)\n",
    "        for i, p, g, m, v in zip(range(len(params)), params, grads, self.ms, self.vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * np.square(g)\n",
    "            p_t = p - lr_t * m_t / (np.sqrt(v_t) + self.epsilon)\n",
    "            self.ms[i] = m_t\n",
    "            self.vs[i] = v_t\n",
    "            ret[i] = p_t\n",
    "        \n",
    "        self.iterations += 1\n",
    "        \n",
    "        for i in range(len(ret)):\n",
    "            ret[i] = ret[i].reshape(original_shapes[i])\n",
    "        \n",
    "        return ret\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"SGD optimizer.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, **kwargs):\n",
    "        \n",
    "        allowed_kwargs = {'clipnorm', 'clipvalue'}\n",
    "        for k in kwargs:\n",
    "            if k not in allowed_kwargs:\n",
    "                raise TypeError('Unexpected keyword argument '\n",
    "                                'passed to optimizer: ' + str(k))\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.lr = lr\n",
    "\n",
    "    def get_update(self, params, grads):\n",
    "        \"\"\" params and grads are list of numpy arrays\n",
    "        \"\"\"\n",
    "        original_shapes = [x.shape for x in params]\n",
    "        params = [x.flatten() for x in params]\n",
    "        grads = [x.flatten() for x in grads]\n",
    "        \n",
    "        \"\"\" #TODO: implement clipping\n",
    "        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n",
    "            norm = np.sqrt(sum([np.sum(np.square(g)) for g in grads]))\n",
    "            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n",
    "        if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n",
    "            grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n",
    "        \"\"\"\n",
    "        \n",
    "        ret = [None] * len(params)\n",
    "        for i, p, g in zip(range(len(params)), params, grads):\n",
    "            ret[i] = p - self.lr * g\n",
    "        \n",
    "        for i in range(len(ret)):\n",
    "            ret[i] = ret[i].reshape(original_shapes[i])\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Corex SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install CUDA and cudamat (for python) to enable GPU speedups.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Linear Total Correlation Explanation\n",
    "Recovers linear latent factors from data, like PCA/ICA/FA, etc. except that\n",
    "these factors are maximally informative about relationships in the data.\n",
    "We also constrain our solutions to be \"non-synergistic\" for better interpretability.\n",
    "(That is the TC(Y|Xi)=0 constraint in the \"blessing of dimensionality\" paper.)\n",
    "Code below written by:\n",
    "Greg Ver Steeg (gregv@isi.edu), 2017.\n",
    "\"\"\"\n",
    "\n",
    "from scipy.stats import norm, rankdata\n",
    "import gc\n",
    "try:\n",
    "    import cudamat as cm\n",
    "    GPU_SUPPORT = True\n",
    "except:\n",
    "    print(\"Install CUDA and cudamat (for python) to enable GPU speedups.\")\n",
    "    GPU_SUPPORT = False\n",
    "\n",
    "\n",
    "class Corex(object):\n",
    "    \"\"\"\n",
    "    Linear Total Correlation Explanation\n",
    "    Conventions\n",
    "    ----------\n",
    "    Code follows sklearn naming/style (e.g. fit(X) to train, transform() to apply model to test data).\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_hidden : int, default = 2\n",
    "        The number of latent factors to use.\n",
    "    max_iter : int, default = 10000\n",
    "        The max. number of iterations to reach convergence.\n",
    "    tol : float, default = 0.0001\n",
    "        Used to test for convergence.\n",
    "    eliminate_synergy : bool, default = True\n",
    "        Use a constraint that the information latent factors have about data is not synergistic.\n",
    "    gaussianize : str, default = 'standard'\n",
    "        Preprocess data so each marginal is near a standard normal. See gaussianize method for more details.\n",
    "    yscale : float default = 1\n",
    "        We imagine some small fundamental measurement noise on Y. The value is arbitrary, but it sets\n",
    "        the scale of the results, Y.\n",
    "    verbose : int, optional\n",
    "        Print verbose outputs.\n",
    "    seed : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    References\n",
    "    ----------\n",
    "    [1] Greg Ver Steeg and Aram Galstyan. \"Maximally Informative Hierarchical...\", AISTATS 2015.\n",
    "    [2] Greg Ver Steeg, Shuyang Gao, Kyle Reing, and Aram Galstyan. \"Sifting Common Information from Many Variables\",\n",
    "                                                                    IJCAI 2017.\n",
    "    [3] Greg Ver Steeg and Aram Galstyan. \"Low Complexity Gaussian Latent Factor Models and\n",
    "                                           a Blessing of Dimensionality\", 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_hidden=10, max_iter=10000, tol=1e-5, anneal=True, missing_values=None,\n",
    "                 discourage_overlap=True, gaussianize='standard', gpu=False,\n",
    "                 verbose=False, seed=None, optimizer=SGD(lr=1e-2)):\n",
    "        \n",
    "        self.m = n_hidden  # Number of latent factors to learn\n",
    "        self.max_iter = max_iter  # Number of iterations to try\n",
    "        self.tol = tol  # Threshold for convergence\n",
    "        self.anneal = anneal\n",
    "        self.eps = 0  # If anneal is True, it's adjusted during optimization to avoid local minima\n",
    "        self.missing_values = missing_values\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.discourage_overlap = discourage_overlap  # Whether or not to discourage overlapping latent factors\n",
    "        self.gaussianize = gaussianize  # Preprocess data: 'standard' scales to zero mean and unit variance\n",
    "        self.gpu = gpu  # Enable GPU support for some large matrix multiplications.\n",
    "        if self.gpu:\n",
    "            cm.cublas_init()\n",
    "\n",
    "        self.yscale = 1.  # Can be arbitrary, but sets the scale of Y\n",
    "        np.random.seed(seed)  # Set seed for deterministic results\n",
    "        self.verbose = verbose\n",
    "        if verbose:\n",
    "            np.set_printoptions(precision=3, suppress=True, linewidth=160)\n",
    "            print('Linear CorEx with {:d} latent factors'.format(n_hidden))\n",
    "\n",
    "        # Initialize these when we fit on data\n",
    "        self.n_samples, self.nv = 0, 0  # Number of samples/variables in input data\n",
    "        self.ws = np.zeros((0, 0))  # m by nv array of weights\n",
    "        self.moments = {}  # Dictionary of moments\n",
    "        self.theta = None  # Parameters for preprocessing each variable\n",
    "        self.history = {}  # Keep track of values for each iteration\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.fit(x)\n",
    "        return self.transform(x)\n",
    "\n",
    "    def fit(self, x):\n",
    "        x = np.asarray(x, dtype=np.float32)\n",
    "        x = self.preprocess(x, fit=True)  # Fit a transform for each marginal\n",
    "        self.n_samples, self.nv = x.shape  # Number of samples, variables in input data\n",
    "        if self.m is None:\n",
    "            self.m = pick_n_hidden(x)\n",
    "        anneal_schedule = [0.]\n",
    "        if self.ws.size == 0:  # Randomly initialize weights if not already set\n",
    "            if self.discourage_overlap:\n",
    "                self.ws = np.random.randn(self.m, self.nv).astype(np.float32)\n",
    "                self.ws /= (10. * self._norm(x, self.ws))[:, np.newaxis]  # TODO: test good IC\n",
    "                if self.anneal:\n",
    "                    anneal_schedule = [0.6**k for k in range(1, 7)] + [0]\n",
    "            else:\n",
    "                self.ws = np.random.randn(self.m, self.nv) * self.yscale ** 2 / np.sqrt(self.nv)\n",
    "        self.moments = self._calculate_moments(x, self.ws, quick=True)\n",
    "\n",
    "        for i_eps, eps in enumerate(anneal_schedule):\n",
    "            start_time = time.time()\n",
    "            self.eps = eps\n",
    "            if i_eps > 0:\n",
    "                eps0 = anneal_schedule[i_eps - 1]\n",
    "                mag = (1 - self.yscale**2 / self.moments['Y_j^2']).clip(1e-5)  # May be better to re-initialize un-used latent factors (i.e. yj^2=self.yscale**2)?\n",
    "                wmag = np.sum(self.ws**2, axis=1)\n",
    "                self.ws *= np.sqrt((1 - eps0**2) / (1 - eps**2 - (eps0**2 - eps**2) * wmag / mag))[:, np.newaxis]\n",
    "            self.moments = self._calculate_moments(x, self.ws, quick=True)\n",
    "\n",
    "            for i_loop in range(self.max_iter):\n",
    "                last_tc = self.tc  # Save this TC to compare to possible updates\n",
    "                if self.discourage_overlap:\n",
    "                    self.ws, self.moments = self._update_ns(x)\n",
    "                else:\n",
    "                    self.ws, self.moments = self._update_syn(x, eta=0.1)  # Older method that allows synergies\n",
    "\n",
    "                # assert np.isfinite(self.tc), \"Error: TC is no longer finite: {}\".format(self.tc)\n",
    "                if not self.moments or not np.isfinite(np.sum(self.tc)):\n",
    "                    try:\n",
    "                        print(\"Error: TC is no longer finite: {}\".format(np.sum(self.tc)))\n",
    "                    except:\n",
    "                        print(\"Error... updates giving invalid solutions?\")\n",
    "                        return self\n",
    "                delta = np.abs(np.sum(self.tc) - last_tc)\n",
    "                self.update_records(self.moments, delta)  # Book-keeping\n",
    "                if delta < self.tol:  # Check for convergence\n",
    "                    if self.verbose:\n",
    "                        print('{:d} iterations to tol: {:f}, Time: {}'.format(i_loop, self.tol,\n",
    "                                                                              time.time() - start_time))\n",
    "                    break\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(\"Warning: Convergence not achieved in {:d} iterations. \"\n",
    "                          \"Final delta: {:f}, Time: {}\".format(self.max_iter, delta.sum(),\n",
    "                                                               time.time() - start_time))\n",
    "        self.moments = self._calculate_moments(x, self.ws, quick=False)  # Update moments with details\n",
    "        order = np.argsort(-self.moments[\"TCs\"])  # Largest TC components first.\n",
    "        self.ws = self.ws[order]\n",
    "        self.moments = self._calculate_moments(x, self.ws, quick=False)  # Update moments based on sorted weights.\n",
    "        return self\n",
    "\n",
    "    def update_records(self, moments, delta):\n",
    "        \"\"\"Print and store some statistics about each iteration.\"\"\"\n",
    "        gc.disable()  # There's a bug that slows when appending, fixed by temporarily disabling garbage collection\n",
    "        self.history[\"TC\"] = self.history.get(\"TC\", []) + [moments[\"TC\"]]\n",
    "        if self.verbose > 1:\n",
    "            print(\"TC={:.3f}\\tadd={:.3f}\\tdelta={:.6f}\".format(moments[\"TC\"], moments.get(\"additivity\", 0), delta))\n",
    "        if self.verbose:\n",
    "            self.history[\"additivity\"] = self.history.get(\"additivity\", []) + [moments.get(\"additivity\", 0)]\n",
    "            self.history[\"TCs\"] = self.history.get(\"TCs\", []) + [moments.get(\"TCs\", np.zeros(self.m))]\n",
    "        gc.enable()\n",
    "\n",
    "    @property\n",
    "    def tc(self):\n",
    "        \"\"\"This actually returns the lower bound on TC that is optimized. The lower bound assumes a constraint that\n",
    "         would be satisfied by a non-overlapping model.\n",
    "         Check \"moments\" for two other estimates of TC that may be useful.\"\"\"\n",
    "        return self.moments[\"TC\"]\n",
    "\n",
    "    @property\n",
    "    def tcs(self):\n",
    "        \"\"\"TCs for each individual latent factor. They DO NOT sum to TC in this case, because of overlaps.\"\"\"\n",
    "        return self.moments[\"TCs\"]\n",
    "\n",
    "    @property\n",
    "    def mis(self):\n",
    "        return - 0.5 * np.log1p(-self.moments[\"rho\"]**2)\n",
    "\n",
    "    def clusters(self):\n",
    "        return np.argmax(np.abs(self.ws), axis=0)\n",
    "\n",
    "    def _sig(self, x, u):\n",
    "        \"\"\"Multiple the matrix u by the covariance matrix of x. We are interested in situations where\n",
    "        n_variables >> n_samples, so we do this without explicitly constructing the covariance matrix.\"\"\"\n",
    "        if self.gpu:\n",
    "            y = cm.empty((self.n_samples, self.m))\n",
    "            uc = cm.CUDAMatrix(u)\n",
    "            cm.dot(x, uc.T, target=y)\n",
    "            del uc\n",
    "            tmp = cm.empty((self.nv, self.m))\n",
    "            cm.dot(x.T, y, target=tmp)\n",
    "            tmp_dot = tmp.asarray()\n",
    "            del y\n",
    "            del tmp\n",
    "        else:\n",
    "            y = x.dot(u.T)\n",
    "            tmp_dot = x.T.dot(y)\n",
    "        prod = (1 - self.eps**2) * tmp_dot.T / self.n_samples + self.eps**2 * u  # nv by m,  <X_i Y_j> / std Y_j\n",
    "        return prod\n",
    "\n",
    "    def _norm(self, x, ws):\n",
    "        \"\"\"Calculate uj so that we can normalize it.\"\"\"\n",
    "        if self.gpu:\n",
    "            y = cm.empty((self.n_samples, self.m))\n",
    "            wc = cm.CUDAMatrix(ws)\n",
    "            cm.dot(x, wc.T, target=y)  # + noise, but it is included analytically\n",
    "            y_local = y.asarray()\n",
    "            del y\n",
    "            del wc\n",
    "            tmp_sum = np.einsum('lj,lj->j', y_local, y_local)  # TODO: Should be able to do on gpu...\n",
    "        else:\n",
    "            y = x.dot(ws.T)  # + noise / std Y_j^2, but it is included analytically\n",
    "            tmp_sum = np.einsum('lj,lj->j', y, y)\n",
    "        return np.sqrt((1 - self.eps**2) * tmp_sum / self.n_samples + self.eps**2 * np.sum(ws**2, axis=1))\n",
    "\n",
    "    def _calculate_moments(self, x, ws, quick=False):\n",
    "        if self.discourage_overlap:\n",
    "            return self._calculate_moments_ns(x, ws, quick=quick)\n",
    "        else:\n",
    "            return self._calculate_moments_syn(x, ws, quick=quick)\n",
    "\n",
    "    def _calculate_moments_ns(self, x, ws, quick=False):\n",
    "        \"\"\"Calculate moments based on the weights and samples. We also calculate and save MI, TC, additivity, and\n",
    "        the value of the objective. Note it is assumed that <X_i^2> = 1! \"\"\"\n",
    "        m = {}  # Dictionary of moments\n",
    "        if self.gpu:\n",
    "            y = cm.empty((self.n_samples, self.m))\n",
    "            wc = cm.CUDAMatrix(ws)\n",
    "            cm.dot(x, wc.T, target=y)  # + noise, but it is included analytically\n",
    "            del wc\n",
    "            tmp_sum = np.einsum('lj,lj->j', y.asarray(), y.asarray())  # TODO: Should be able to do on gpu...\n",
    "        else:\n",
    "            y = x.dot(ws.T)\n",
    "            tmp_sum = np.einsum('lj,lj->j', y, y)\n",
    "        m[\"uj\"] = (1 - self.eps**2) * tmp_sum / self.n_samples + self.eps**2 * np.sum(ws**2, axis=1)\n",
    "#         if quick and np.max(m[\"uj\"]) >= 1.:\n",
    "#             return False\n",
    "        if self.gpu:\n",
    "            tmp = cm.empty((self.nv, self.m))\n",
    "            cm.dot(x.T, y, target=tmp)\n",
    "            tmp_dot = tmp.asarray()\n",
    "            del tmp\n",
    "            del y\n",
    "        else:\n",
    "            tmp_dot = x.T.dot(y)\n",
    "        m[\"rho\"] = (1 - self.eps**2) * tmp_dot.T / self.n_samples + self.eps**2 * ws  # m by nv\n",
    "        m[\"ry\"] = ws.dot(m[\"rho\"].T)  # normalized covariance of Y\n",
    "        m[\"Y_j^2\"] = self.yscale ** 2 / (1. - m[\"uj\"])\n",
    "        np.fill_diagonal(m[\"ry\"], 1)\n",
    "        m[\"invrho\"] = 1. / (1. - m[\"rho\"]**2)\n",
    "        m[\"rhoinvrho\"] = m[\"rho\"] * m[\"invrho\"]\n",
    "        m[\"Qij\"] = np.dot(m['ry'], m[\"rhoinvrho\"])\n",
    "        m[\"Qi\"] = np.einsum('ki,ki->i', m[\"rhoinvrho\"], m[\"Qij\"])\n",
    "        #m[\"Qi-Si^2\"] = np.einsum('ki,ki->i', m[\"rhoinvrho\"], m[\"Qij\"])\n",
    "        m[\"Si\"] = np.sum(m[\"rho\"] * m[\"rhoinvrho\"], axis=0)\n",
    "\n",
    "        # This is the objective, a lower bound for TC\n",
    "        m[\"TC\"] = np.sum(np.log(1 + m[\"Si\"])) \\\n",
    "                     - 0.5 * np.sum(np.log(1 - m[\"Si\"]**2 + m[\"Qi\"])) \\\n",
    "                     + 0.5 * np.sum(np.log(1 - m[\"uj\"]))\n",
    "\n",
    "        if not quick:\n",
    "            m[\"MI\"] = - 0.5 * np.log1p(-m[\"rho\"]**2)\n",
    "            m[\"X_i Y_j\"] = m[\"rho\"].T * np.sqrt(m[\"Y_j^2\"])\n",
    "            m[\"X_i Z_j\"] = np.linalg.solve(m[\"ry\"], m[\"rho\"]).T\n",
    "            m[\"X_i^2 | Y\"] = (1. - np.einsum('ij,ji->i', m[\"X_i Z_j\"], m[\"rho\"])).clip(1e-6)\n",
    "            m['I(Y_j ; X)'] = 0.5 * np.log(m[\"Y_j^2\"]) - 0.5 * np.log(self.yscale ** 2)\n",
    "            m['I(X_i ; Y)'] = - 0.5 * np.log(m[\"X_i^2 | Y\"])\n",
    "            m[\"TCs\"] = m[\"MI\"].sum(axis=1) - m['I(Y_j ; X)']\n",
    "            m[\"TC_no_overlap\"] = m[\"MI\"].max(axis=0).sum() - m['I(Y_j ; X)'].sum()  # A direct calculation of TC where each variable is in exactly one group.\n",
    "            m[\"TC_direct\"] = m['I(X_i ; Y)'].sum() - m['I(Y_j ; X)']  # A direct calculation of TC. Should be upper bound for \"TC\", \"TC_no_overlap\"\n",
    "            m[\"additivity\"] = (m[\"MI\"].sum(axis=0) - m['I(X_i ; Y)']).sum()\n",
    "        return m\n",
    "\n",
    "    def z2_fromW(self, j, W, x):\n",
    "        # \\eta^2 + W_j \\Sigma W_j^T\n",
    "        return self.yscale**2 + np.dot(self._sig(x, W[j:j+1,:]), W[j:j+1, :].T)\n",
    "\n",
    "    def z2_fromU(self, j, U, x):\n",
    "        # \\frac{\\eta^2}{1 - U_j \\Sigma U_j^T}\n",
    "        return (self.yscale**2) / (1 - np.dot(self._sig(x, U[j:j+1,:]), U[j:j+1, :].T))\n",
    "\n",
    "    def getW(self, U, x):\n",
    "        # W_{ji} = U_{ji} * \\sqrt{E[Z_j^2]}\n",
    "        \"\"\"\n",
    "        W = np.zeros(U.shape)\n",
    "        for j in range(U.shape[0]):\n",
    "            W[j, :] = U[j, :] * np.sqrt(self.z2_fromU(j, U, x))\n",
    "        return W\n",
    "        \"\"\"\n",
    "        tmp_dot = np.dot(self._sig(x, U), U.T)\n",
    "        z2 = (self.yscale**2) / (1 - np.einsum(\"ii->i\", tmp_dot))\n",
    "        return U * np.sqrt(z2).reshape((-1, 1))\n",
    "\n",
    "    def getU(self, W, x):\n",
    "        # U_{ji} = \\frac{W_{ji}}{\\sqrt{E[Z_j^2]}}\n",
    "        \"\"\"\n",
    "        U = np.zeros(W.shape)\n",
    "        for j in range(W.shape[0]):\n",
    "            U[j, :] = W[j, :] / np.sqrt(self.z2_fromW(j, W, x))\n",
    "        return U\n",
    "        \"\"\"\n",
    "        tmp_dot = np.dot(self._sig(x, W), W.T)\n",
    "        z2 = self.yscale**2 + np.einsum(\"ii->i\", tmp_dot)\n",
    "        return W / np.sqrt(z2).reshape((-1, 1))\n",
    "    \n",
    "    def _update_ns(self, x, return_gradient=False):\n",
    "        \"\"\"Perform one update of the weights and re-calculate moments in the NON-SYNERGISTIC case.\"\"\"\n",
    "        m = self.moments\n",
    "        rj = 1. - m[\"uj\"][:, np.newaxis]\n",
    "        H = np.dot(m[\"rhoinvrho\"] / (1 + m[\"Qi\"] - m[\"Si\"]**2), m[\"rhoinvrho\"].T)\n",
    "        np.fill_diagonal(H, 0)\n",
    "        grad = self.ws / rj\n",
    "        grad -= 2 * m[\"invrho\"] * m[\"rhoinvrho\"] / (1 + m[\"Si\"])\n",
    "        grad += m[\"invrho\"]**2 * \\\n",
    "               ((1 + m[\"rho\"]**2) * m[\"Qij\"] - 2 * m[\"rho\"] * m[\"Si\"]) / (1 - m[\"Si\"]**2 + m[\"Qi\"])\n",
    "        grad += np.dot(H, self.ws)\n",
    "    \n",
    "        if return_gradient:\n",
    "            return grad\n",
    "        \n",
    "        # make the gradient a bit correct by multipying each roE[zj^2]\n",
    "        \"\"\"\n",
    "        for j in range(self.ws.shape[0]):\n",
    "            t = np.float(self.z2_fromU(j, self.ws, x))\n",
    "            grad[j, :] *= np.sqrt(t)\n",
    "        \"\"\"\n",
    "        tmp_dot = np.dot(self._sig(x, self.ws), self.ws.T)\n",
    "        z2 = (self.yscale**2) / (1 - np.einsum(\"ii->i\", tmp_dot))\n",
    "        grad *= np.sqrt(z2).reshape((-1, 1))\n",
    "        \n",
    "        current_real_W = self.getW(self.ws, x)\n",
    "        new_real_W = self.optimizer.get_update([current_real_W], [grad])[0]\n",
    "        w_update = self.getU(new_real_W, x)\n",
    "        m_update = self._calculate_moments_ns(x, w_update, quick=True)\n",
    "        #print \"TC = {}, eps = {}\".format(self.tc, self.eps)\n",
    "        \n",
    "        return w_update, m_update\n",
    "\n",
    "    def _calculate_moments_syn(self, x, ws, quick=False):\n",
    "        \"\"\"Calculate moments based on the weights and samples. We also calculate and save MI, TC, additivity, and\n",
    "        the value of the objective. Note it is assumed that <X_i^2> = 1! \"\"\"\n",
    "        m = {}  # Dictionary of moments\n",
    "        if self.gpu:\n",
    "            y = cm.empty((self.n_samples, self.m))\n",
    "            wc = cm.CUDAMatrix(ws)\n",
    "            cm.dot(x, wc.T, target=y)  # + noise, but it is included analytically\n",
    "            del wc\n",
    "        else:\n",
    "            y = x.dot(ws.T)  # + noise, but it is included analytically\n",
    "        if self.gpu:\n",
    "            tmp_dot = cm.empty((self.nv, self.m))\n",
    "            cm.dot(x.T, y, target=tmp_dot)\n",
    "            m[\"X_i Y_j\"] = tmp_dot.asarray() / self.n_samples  # nv by m,  <X_i Y_j>\n",
    "            del y\n",
    "            del tmp_dot\n",
    "        else:\n",
    "            m[\"X_i Y_j\"] = x.T.dot(y) / self.n_samples\n",
    "        m[\"cy\"] = ws.dot(m[\"X_i Y_j\"]) + self.yscale ** 2 * np.eye(self.m)  # cov(y.T), m by m\n",
    "        m[\"Y_j^2\"] = np.diag(m[\"cy\"]).copy()\n",
    "        m[\"ry\"] = m[\"cy\"] / (np.sqrt(m[\"Y_j^2\"]) * np.sqrt(m[\"Y_j^2\"][:, np.newaxis]))\n",
    "        m[\"rho\"] = (m[\"X_i Y_j\"] / np.sqrt(m[\"Y_j^2\"])).T\n",
    "        m[\"invrho\"] = 1. / (1. - m[\"rho\"]**2)\n",
    "        m[\"rhoinvrho\"] = m[\"rho\"] * m[\"invrho\"]\n",
    "        m[\"Qij\"] = np.dot(m['ry'], m[\"rhoinvrho\"])\n",
    "        m[\"Qi\"] = np.einsum('ki,ki->i', m[\"rhoinvrho\"], m[\"Qij\"])\n",
    "        m[\"Si\"] = np.sum(m[\"rho\"] * m[\"rhoinvrho\"], axis=0)\n",
    "\n",
    "        m[\"MI\"] = - 0.5 * np.log1p(-m[\"rho\"]**2)\n",
    "        m[\"X_i Z_j\"] = np.linalg.solve(m[\"cy\"], m[\"X_i Y_j\"].T).T\n",
    "        m[\"X_i^2 | Y\"] = (1. - np.einsum('ij,ij->i', m[\"X_i Z_j\"], m[\"X_i Y_j\"])).clip(1e-6)\n",
    "        mi_yj_x = 0.5 * np.log(m[\"Y_j^2\"]) - 0.5 * np.log(self.yscale ** 2)\n",
    "        mi_xi_y = - 0.5 * np.log(m[\"X_i^2 | Y\"])\n",
    "        m[\"TCs\"] = m[\"MI\"].sum(axis=1) - mi_yj_x\n",
    "        m[\"additivity\"] = (m[\"MI\"].sum(axis=0) - mi_xi_y).sum()\n",
    "        m[\"TC\"] = np.sum(mi_xi_y) - np.sum(mi_yj_x)\n",
    "        return m\n",
    "\n",
    "    def _update_syn(self, x, eta=0.5):\n",
    "        \"\"\"Perform one update of the weights and re-calculate moments in the SYNERGISTIC case.\"\"\"\n",
    "        m = self.moments\n",
    "        H = (1. / m[\"X_i^2 | Y\"] * m[\"X_i Z_j\"].T).dot(m[\"X_i Z_j\"])\n",
    "        np.fill_diagonal(H, 0)\n",
    "        R = m[\"X_i Z_j\"].T / m[\"X_i^2 | Y\"]\n",
    "        S = np.dot(H, self.ws)\n",
    "        ws = (1. - eta) * self.ws + eta * (R - S)\n",
    "        m = self._calculate_moments_syn(x, ws)\n",
    "        return ws, m\n",
    "\n",
    "    def transform(self, x, details=False):\n",
    "        \"\"\"Transform an array of inputs, x, into an array of k latent factors, Y.\n",
    "            Optionally, you can get the remainder information and/or stop at a specified level.\"\"\"\n",
    "        x = self.preprocess(x)\n",
    "        ns, nv = x.shape\n",
    "        assert self.nv == nv, \"Incorrect number of variables in input, %d instead of %d\" % (nv, self.nv)\n",
    "        if details:\n",
    "            moments = self._calculate_moments(x, self.ws)\n",
    "            return x.dot(self.ws.T), moments\n",
    "        return x.dot(self.ws.T)\n",
    "\n",
    "    def preprocess(self, x, fit=False):\n",
    "        \"\"\"Transform each marginal to be as close to a standard Gaussian as possible.\n",
    "        'standard' (default) just subtracts the mean and scales by the std.\n",
    "        'empirical' does an empirical gaussianization (but this cannot be inverted).\n",
    "        'outliers' tries to squeeze in the outliers\n",
    "        Any other choice will skip the transformation.\"\"\"\n",
    "        if self.missing_values is not None:\n",
    "            x, self.n_obs = mean_impute(x, self.missing_values)  # Creates a copy\n",
    "        else:\n",
    "            self.n_obs = len(x)\n",
    "        if self.gaussianize == 'none':\n",
    "            pass\n",
    "        elif self.gaussianize == 'standard':\n",
    "            if fit:\n",
    "                mean = np.mean(x, axis=0)\n",
    "                # std = np.std(x, axis=0, ddof=0).clip(1e-10)\n",
    "                std = np.sqrt(np.sum((x - mean)**2, axis=0) / self.n_obs).clip(1e-10)\n",
    "                self.theta = (mean, std)\n",
    "            x = ((x - self.theta[0]) / self.theta[1])\n",
    "            if np.max(np.abs(x)) > 6 and self.verbose:\n",
    "                print(\"Warning: outliers more than 6 stds away from mean. Consider using gaussianize='outliers'\")\n",
    "        elif self.gaussianize == 'outliers':\n",
    "            if fit:\n",
    "                mean = np.mean(x, axis=0)\n",
    "                std = np.std(x, axis=0, ddof=0).clip(1e-10)\n",
    "                self.theta = (mean, std)\n",
    "            x = g((x - self.theta[0]) / self.theta[1])  # g truncates long tails\n",
    "        elif self.gaussianize == 'empirical':\n",
    "            print(\"Warning: correct inversion/transform of empirical gauss transform not implemented.\")\n",
    "            x = np.array([norm.ppf((rankdata(x_i) - 0.5) / len(x_i)) for x_i in x.T]).T\n",
    "        if self.gpu and fit:  # Don't return GPU matrices when only transforming\n",
    "            x = cm.CUDAMatrix(x)\n",
    "        return x\n",
    "\n",
    "    def invert(self, x):\n",
    "        \"\"\"Invert the preprocessing step to get x's in the original space.\"\"\"\n",
    "        if self.gaussianize == 'standard':\n",
    "            return self.theta[1] * x + self.theta[0]\n",
    "        elif self.gaussianize == 'outliers':\n",
    "            return self.theta[1] * g_inv(x) + self.theta[0]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def predict(self, y):\n",
    "        return self.invert(np.dot(self.moments[\"X_i Z_j\"], y.T).T)\n",
    "\n",
    "    def get_covariance(self):\n",
    "        # This uses E(Xi|Y) formula for non-synergistic relationships\n",
    "        m = self.moments\n",
    "        if self.discourage_overlap:\n",
    "            z = m['rhoinvrho'] / (1 + m['Si'])\n",
    "            cov = np.dot(z.T, z)\n",
    "            cov /= (1. - self.eps**2)\n",
    "            np.fill_diagonal(cov, 1)\n",
    "            return self.theta[1][:, np.newaxis] * self.theta[1] * cov\n",
    "        else:\n",
    "            cov = np.einsum('ij,kj->ik', m[\"X_i Z_j\"], m[\"X_i Y_j\"])\n",
    "            np.fill_diagonal(cov, 1)\n",
    "            return self.theta[1][:, np.newaxis] * self.theta[1] * cov\n",
    "\n",
    "\n",
    "def pick_n_hidden(data, repeat=1, verbose=False):\n",
    "    \"\"\"A helper function to pick the number of hidden factors / clusters to use.\"\"\"\n",
    "    # TODO: Use an efficient search strategy\n",
    "    max_score = - np.inf\n",
    "    n = 1\n",
    "    while True:\n",
    "        scores = []\n",
    "        for _ in range(repeat):\n",
    "            out = Corex(n_hidden=n, max_iter=1000, tol=1e-3, gpu=False).fit(data)\n",
    "            m = out.moments\n",
    "            scores.append(m[\"TC_no_overlap\"])\n",
    "        score = max(scores)\n",
    "        if verbose:\n",
    "            print((\"n: {}, score: {}\".format(n, score)))\n",
    "        if score < max_score:\n",
    "            break\n",
    "        else:\n",
    "            n += 1\n",
    "            max_score = score\n",
    "    return n - 1\n",
    "\n",
    "\n",
    "def g(x, t=4):\n",
    "    \"\"\"A transformation that suppresses outliers for a standard normal.\"\"\"\n",
    "    xp = np.clip(x, -t, t)\n",
    "    diff = np.tanh(x - xp)\n",
    "    return xp + diff\n",
    "\n",
    "\n",
    "def g_inv(x, t=4):\n",
    "    \"\"\"Inverse of g transform.\"\"\"\n",
    "    xp = np.clip(x, -t, t)\n",
    "    diff = np.arctanh(np.clip(x - xp, -1 + 1e-10, 1 - 1e-10))\n",
    "    return xp + diff\n",
    "\n",
    "\n",
    "def mean_impute(x, v):\n",
    "    \"\"\"Missing values in the data, x, are indicated by v. Wherever this value appears in x, it is replaced by the\n",
    "    mean value taken from the marginal distribution of that column.\"\"\"\n",
    "    if not np.isnan(v):\n",
    "        x = np.where(x == v, np.nan, x)\n",
    "    x_new = []\n",
    "    n_obs = []\n",
    "    for i, xi in enumerate(x.T):\n",
    "        missing_locs = np.where(np.isnan(xi))[0]\n",
    "        xi_nm = xi[np.isfinite(xi)]\n",
    "        xi[missing_locs] = np.mean(xi_nm)\n",
    "        x_new.append(xi)\n",
    "        n_obs.append(len(xi_nm))\n",
    "    return np.array(x_new).T, np.array(n_obs)\n",
    "\n",
    "\n",
    "def random_impute(x, v):\n",
    "    \"\"\"Missing values in the data, x, are indicated by v. Wherever this value appears in x, it is replaced by a\n",
    "    random value taken from the marginal distribution of that column.\"\"\"\n",
    "    if not np.isnan(v):\n",
    "        x = np.where(x == v, np.nan, x)\n",
    "    x_new = []\n",
    "    for i, xi in enumerate(x.T):\n",
    "        missing_locs = np.where(np.isnan(xi))[0]\n",
    "        xi_nm = xi[np.isfinite(xi)]\n",
    "        xi[missing_locs] = np.random.choice(xi_nm, size=len(missing_locs))\n",
    "        x_new.append(xi)\n",
    "    return np.array(x_new).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data.shape = (887, 5038)\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    with open('../data/EOD_week.pkl', 'rb') as f:\n",
    "        df = pd.DataFrame(pkl.load(f))\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "print(\"Data.shape = {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corex_params = {\n",
    "    'n_hidden':10,\n",
    "    'max_iter':300, # NOTE: 1e4\n",
    "    'tol':1e-5,\n",
    "    'anneal':True,\n",
    "    'missing_values':None,\n",
    "    'discourage_overlap':True,\n",
    "    'gaussianize':'standard',\n",
    "    'gpu':False,\n",
    "    'verbose':True,\n",
    "    'seed':None,\n",
    "    'optimizer': Adam()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit corex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CorEx with 10 latent factors\n",
      "Warning: outliers more than 6 stds away from mean. Consider using gaussianize='outliers'\n",
      "Warning: Convergence not achieved in 300 iterations. Final delta: 0.017159, Time: 9.55400204659\n",
      "Warning: Convergence not achieved in 300 iterations. Final delta: 0.001276, Time: 13.2356920242\n",
      "Warning: Convergence not achieved in 300 iterations. Final delta: 0.000606, Time: 15.1686401367\n",
      "Warning: Convergence not achieved in 300 iterations. Final delta: 0.000239, Time: 15.1810910702\n",
      "Warning: Convergence not achieved in 300 iterations. Final delta: 0.000222, Time: 15.5165319443\n",
      "298 iterations to tol: 0.000010, Time: 14.8469231129\n",
      "7 iterations to tol: 0.000010, Time: 0.592993021011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Corex at 0x7f0a1b8da050>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corex = Corex(**corex_params)\n",
    "corex.fit(df[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649.65050684561425"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corex.tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W=corex.getW(corex.ws, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U=corex.getW(W, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5038)\n",
      "(10, 5038)\n"
     ]
    }
   ],
   "source": [
    "print U.shape\n",
    "print corex.ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000160770544155\n",
      "2.37129816193e-06\n",
      "0.000800152451868\n",
      "0.0017129099536\n"
     ]
    }
   ],
   "source": [
    "print np.max(np.abs(U-corex.ws))\n",
    "print np.mean(np.abs(U-corex.ws))\n",
    "print np.mean(np.abs(corex.ws))\n",
    "print np.mean(np.abs(U-corex.ws) / np.abs(corex.ws + 1e-13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faa1b2b0710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(corex.get_covariance()[:20, :20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corex = Corex(**corex_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corex.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corex.moments = corex._calculate_moments_ns(X, corex.ws)\n",
    "grad = corex._update_ns(X, return_gradient=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_tc = corex.tc\n",
    "print old_tc, corex._calculate_moments_ns(X, corex.ws)[\"TC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_grad = np.zeros(corex.ws.shape)\n",
    "delta = 1e-8\n",
    "for j in range(corex.m):\n",
    "    print \"\\rj={}\".format(j),\n",
    "    for i in range(corex.nv):\n",
    "        corex.ws[j, i] += delta\n",
    "        new_tc = corex._calculate_moments_ns(X, corex.ws)['TC']\n",
    "        #print(\"numerical gradient = {}, corex.grad = {}\".format((new_tc - old_tc) / delta, grad[j][i]))\n",
    "        num_grad[j,i] = (new_tc - old_tc) / delta\n",
    "        corex.ws[j, i] -= delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(num_grad / old_grad)[:, :20])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
